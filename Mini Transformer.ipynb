{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1)).permute(0,2,1)\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "querries, keys, values, ctx_vecs = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querries.shape, keys.shape, values.shape, ctx_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2804,  0.5018, -0.7626,  0.0181],\n",
       "         [ 0.4935, -0.0438,  0.1872, -0.3386],\n",
       "         [ 0.4075,  0.2587, -0.2906, -0.1660]],\n",
       "\n",
       "        [[-0.2464, -0.2251,  0.3710, -0.0988],\n",
       "         [-0.1341, -0.1451,  0.2347, -0.1433],\n",
       "         [-0.1541, -0.1500,  0.2305, -0.1329]],\n",
       "\n",
       "        [[ 0.0843, -0.2047,  0.1024,  0.1585],\n",
       "         [ 0.0067, -0.3096,  0.1880,  0.1146],\n",
       "         [ 0.1305, -0.1400,  0.0396,  0.2153]],\n",
       "\n",
       "        [[-0.4525, -0.3964,  0.3201, -0.1685],\n",
       "         [-0.4658, -0.4039,  0.3224, -0.1578],\n",
       "         [-0.5057, -0.4148,  0.3209, -0.1241]],\n",
       "\n",
       "        [[ 0.0791,  0.1095, -0.2467,  0.0517],\n",
       "         [ 0.1067,  0.0871, -0.2460,  0.0682],\n",
       "         [ 0.0070,  0.3619, -0.5443,  0.2537]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2804,  0.5018, -0.7626,  0.0181],\n",
       "         [ 0.4935, -0.0438,  0.1872, -0.3386],\n",
       "         [ 0.4075,  0.2587, -0.2906, -0.1660]],\n",
       "\n",
       "        [[-0.2464, -0.2251,  0.3710, -0.0988],\n",
       "         [-0.1341, -0.1451,  0.2347, -0.1433],\n",
       "         [-0.1541, -0.1500,  0.2305, -0.1329]],\n",
       "\n",
       "        [[ 0.0843, -0.2047,  0.1024,  0.1585],\n",
       "         [ 0.0067, -0.3096,  0.1880,  0.1146],\n",
       "         [ 0.1305, -0.1400,  0.0396,  0.2153]],\n",
       "\n",
       "        [[-0.4525, -0.3964,  0.3201, -0.1685],\n",
       "         [-0.4658, -0.4039,  0.3224, -0.1578],\n",
       "         [-0.5057, -0.4148,  0.3209, -0.1241]],\n",
       "\n",
       "        [[ 0.0791,  0.1095, -0.2467,  0.0517],\n",
       "         [ 0.1067,  0.0871, -0.2460,  0.0682],\n",
       "         [ 0.0070,  0.3619, -0.5443,  0.2537]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [self_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = torch.cat([head(x)[3] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "multihead = multi_head_attn(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = multihead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3967, -0.4712, -0.7870,  0.4672,  1.6398,  0.5479],\n",
       "         [-0.9126,  0.7557, -1.0113,  1.1812, -1.0501,  1.0370],\n",
       "         [-0.3855, -0.5054,  2.0535,  0.3557, -0.5357, -0.9826]],\n",
       "\n",
       "        [[-1.6022,  0.9865,  1.3913,  0.0374, -0.0966, -0.7164],\n",
       "         [-0.0444,  1.2961,  0.6694,  0.3283, -0.3399, -1.9095],\n",
       "         [-0.6499, -0.5765,  0.4959, -0.8299,  2.0238, -0.4634]],\n",
       "\n",
       "        [[-0.0690, -1.8813,  1.2001, -0.3236,  0.9458,  0.1280],\n",
       "         [ 0.7996, -0.6632,  0.8283, -0.2290,  1.0307, -1.7663],\n",
       "         [-1.1234,  1.3501, -1.3677,  0.9640,  0.3114, -0.1345]],\n",
       "\n",
       "        [[-1.7464, -0.4007,  0.6703, -0.3558,  0.3994,  1.4332],\n",
       "         [ 0.3436, -1.4343,  1.6375, -0.9994,  0.3713,  0.0814],\n",
       "         [ 0.7205, -2.0331,  0.0393,  0.9465,  0.6087, -0.2819]],\n",
       "\n",
       "        [[ 1.1007,  0.5273, -0.2811,  1.1312, -1.0375, -1.4406],\n",
       "         [-0.6268, -0.2386,  0.2656,  0.6721, -1.6213,  1.5490],\n",
       "         [-0.0276, -0.2385, -1.3998,  1.7999, -0.6727,  0.5388]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(multihead.LNorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9671e-08,  1.9868e-08,  3.9736e-08],\n",
       "        [ 3.9736e-08, -3.9736e-08,  1.9868e-08],\n",
       "        [-3.2286e-08,  0.0000e+00, -2.9802e-08],\n",
       "        [-1.9868e-08,  8.6923e-09,  1.9868e-08],\n",
       "        [ 1.9868e-08,  0.0000e+00, -9.9341e-09]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.mean(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954]], grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.std(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
