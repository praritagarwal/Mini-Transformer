{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1)).permute(0,2,1)\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "querries, keys, values, ctx_vecs = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querries.shape, keys.shape, values.shape, ctx_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0695,  0.1027, -0.1141, -0.0260],\n",
       "         [ 0.1070,  0.0183, -0.1150,  0.0011],\n",
       "         [ 0.1134, -0.0138, -0.1107,  0.0092]],\n",
       "\n",
       "        [[-0.1350,  0.8512,  0.0235, -0.4540],\n",
       "         [-0.1487,  0.8075,  0.0171, -0.4436],\n",
       "         [-0.1452,  0.8116,  0.0182, -0.4431]],\n",
       "\n",
       "        [[ 0.1837, -0.4192, -0.0492,  0.3628],\n",
       "         [ 0.2332, -0.3694, -0.0935,  0.3753],\n",
       "         [ 0.2078, -0.3799, -0.0827,  0.3814]],\n",
       "\n",
       "        [[ 0.2248, -0.1568,  0.0170,  0.0817],\n",
       "         [ 0.2360, -0.1490,  0.0138,  0.0803],\n",
       "         [ 0.2438, -0.1497,  0.0096,  0.0782]],\n",
       "\n",
       "        [[-0.2843, -0.2287,  0.2798,  0.0842],\n",
       "         [-0.2868, -0.2681,  0.2785,  0.1049],\n",
       "         [-0.2441, -0.1056,  0.2685,  0.0219]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0695,  0.1027, -0.1141, -0.0260],\n",
       "         [ 0.1070,  0.0183, -0.1150,  0.0011],\n",
       "         [ 0.1134, -0.0138, -0.1107,  0.0092]],\n",
       "\n",
       "        [[-0.1350,  0.8512,  0.0235, -0.4540],\n",
       "         [-0.1487,  0.8075,  0.0171, -0.4436],\n",
       "         [-0.1452,  0.8116,  0.0182, -0.4431]],\n",
       "\n",
       "        [[ 0.1837, -0.4192, -0.0492,  0.3628],\n",
       "         [ 0.2332, -0.3694, -0.0935,  0.3753],\n",
       "         [ 0.2078, -0.3799, -0.0827,  0.3814]],\n",
       "\n",
       "        [[ 0.2248, -0.1568,  0.0170,  0.0817],\n",
       "         [ 0.2360, -0.1490,  0.0138,  0.0803],\n",
       "         [ 0.2438, -0.1497,  0.0096,  0.0782]],\n",
       "\n",
       "        [[-0.2843, -0.2287,  0.2798,  0.0842],\n",
       "         [-0.2868, -0.2681,  0.2785,  0.1049],\n",
       "         [-0.2441, -0.1056,  0.2685,  0.0219]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [self_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = torch.cat([head(x)[3] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "multihead = multi_head_attn(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = multihead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3970, -0.2556, -0.4025, -1.2867,  0.3908,  1.9510],\n",
       "         [-1.0594,  0.7616,  1.2213, -1.5387,  0.6606, -0.0454],\n",
       "         [-1.9424,  0.8693, -0.0473, -0.4243,  0.9915,  0.5532]],\n",
       "\n",
       "        [[ 0.3326,  0.4952, -2.1718,  0.8860,  0.0920,  0.3660],\n",
       "         [ 0.9578, -1.8795, -0.5415, -0.0991,  0.8968,  0.6655],\n",
       "         [-0.9096, -1.4197,  0.7999,  1.3254,  0.7101, -0.5061]],\n",
       "\n",
       "        [[-0.1232,  0.9751,  0.1593,  0.8051,  0.2561, -2.0724],\n",
       "         [-0.1782, -0.4761,  0.8225,  0.6757,  1.0362, -1.8801],\n",
       "         [-0.8773,  0.5034, -0.4839, -0.2464, -0.8748,  1.9791]],\n",
       "\n",
       "        [[ 0.1125, -1.9037,  0.3175, -0.1553,  1.4896,  0.1396],\n",
       "         [-1.5095,  0.8640, -1.1197, -0.0211,  0.6430,  1.1433],\n",
       "         [-1.2516,  0.8050,  0.4536, -1.5385,  0.9071,  0.6243]],\n",
       "\n",
       "        [[ 0.7306, -1.5146, -1.0940,  0.0747,  1.3162,  0.4870],\n",
       "         [ 1.7558,  0.5619, -0.2374, -0.4283, -0.1200, -1.5320],\n",
       "         [-0.0811,  0.6744,  1.7611, -1.0020, -0.1673, -1.1851]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(multihead.LNorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  9.3132e-09, -9.9341e-09],\n",
       "        [ 0.0000e+00, -9.9341e-09,  1.9868e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.4835e-09, -1.9868e-08, -2.9802e-08],\n",
       "        [ 1.4901e-08, -5.9605e-08,  1.5895e-07]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.mean(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954]], grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.std(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    '''\n",
    "    The complete encoder module.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, emb_dim, h, parallelize = False, ffn_l1_out_fts = 2048 ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, parallelize)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = self.mul_h_attn(x)\n",
    "        out = torch.relu(self.l1(ctx_vecs))\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return self.LNorm(out + x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "enc = encoder(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (l1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6807, -0.4590,  1.8553, -0.4325, -0.4071, -1.2374],\n",
       "         [ 1.1996,  0.5036, -1.2591, -0.8528,  1.1819, -0.7732],\n",
       "         [ 0.4374, -1.4284, -1.1448,  0.5949,  0.0936,  1.4473]],\n",
       "\n",
       "        [[-0.6799, -0.3918,  0.5790, -1.6221,  1.3587,  0.7562],\n",
       "         [ 0.5610, -1.5596,  0.0128, -1.0685,  1.0122,  1.0421],\n",
       "         [ 0.2855, -0.9815,  1.1326,  0.3345, -1.6628,  0.8918]],\n",
       "\n",
       "        [[-1.1680, -1.2111, -0.1682,  1.6213,  0.2591,  0.6670],\n",
       "         [-1.5072, -1.1199,  0.5933,  1.0595, -0.0252,  0.9994],\n",
       "         [ 1.0912, -0.6212, -1.1809, -0.9823,  0.2850,  1.4082]],\n",
       "\n",
       "        [[ 1.0300,  0.4345, -2.1187,  0.4893,  0.1470,  0.0179],\n",
       "         [-0.2882, -0.8012, -0.0468, -1.2432,  1.8592,  0.5202],\n",
       "         [ 1.1637, -1.4063,  0.8195, -0.6638,  0.9243, -0.8375]],\n",
       "\n",
       "        [[-1.2617,  0.1805,  1.5193, -0.1089, -1.1649,  0.8356],\n",
       "         [ 1.7452, -0.0804, -0.6855,  0.6709, -0.2481, -1.4021],\n",
       "         [-1.6552,  0.5707,  0.2026,  0.8890,  1.0219, -1.0290]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = enc(x)\n",
    "enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder_attention(nn.Module):\n",
    "    '''\n",
    "    Module to implement the encoder_decoder attention layer. \n",
    "    This is same as the self_attention layer except that it takes two input vectors: \n",
    "                 1)encoder's final output \n",
    "                 2) output from previous decoder layer\n",
    "    The querries are generated from the previous decoder layer's output\n",
    "    The keys and the values are generated from the encoder's output \n",
    "         \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Key vector\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Value vector\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = enc_out.shape[0]\n",
    "        seq_len = enc_out.shape[1]\n",
    "        querries = self.WQ(dec_out)\n",
    "        keys = self.WK(enc_out)\n",
    "        values = self.WV(enc_out)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1))\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_decoder_attention(\n",
       "  (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = encoder_decoder_attention(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, s, c = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 4]),\n",
       " torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1055, 1.0679, 0.7241], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = q[0,0]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = k[0]\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2760, -0.6922, -0.5533, -0.9880], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760, -0.6922, -0.5533, -0.9880],\n",
       "         [ 0.0452, -0.1738, -0.6367,  1.0138],\n",
       "         [-0.2108, -0.0797, -0.5803,  0.6809],\n",
       "         [ 0.8407,  0.5261,  1.1788, -1.1274]],\n",
       "\n",
       "        [[-0.0912,  0.8676, -0.7004, -0.8413],\n",
       "         [-0.0349, -0.4209,  0.2272,  0.2838],\n",
       "         [-0.2665,  1.3260, -0.7065, -0.8829],\n",
       "         [-0.1835, -1.0982,  0.4689,  0.6033]],\n",
       "\n",
       "        [[-1.6119,  0.5404, -0.3440,  0.3651],\n",
       "         [-0.4196,  0.3210, -0.4347,  0.3786],\n",
       "         [ 0.1711, -0.1909,  0.4034, -0.3576],\n",
       "         [-0.4332,  0.1890,  0.4023, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1933,  0.1216, -0.4201],\n",
       "         [-0.1057, -0.7515, -0.7439,  0.2262],\n",
       "         [-0.0205, -0.6539, -1.1184,  0.0809],\n",
       "         [-0.3543, -0.2760,  1.5729,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.2947, -0.5404, -0.0393],\n",
       "         [-0.3528, -0.3723,  1.6815, -0.4538],\n",
       "         [ 0.5769,  0.3728,  0.1997,  0.2831],\n",
       "         [ 0.1172,  0.3279,  0.6438, -0.0655]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760,  0.0452, -0.2108,  0.8407],\n",
       "         [-0.6922, -0.1738, -0.0797,  0.5261],\n",
       "         [-0.5533, -0.6367, -0.5803,  1.1788],\n",
       "         [-0.9880,  1.0138,  0.6809, -1.1274]],\n",
       "\n",
       "        [[-0.0912, -0.0349, -0.2665, -0.1835],\n",
       "         [ 0.8676, -0.4209,  1.3260, -1.0982],\n",
       "         [-0.7004,  0.2272, -0.7065,  0.4689],\n",
       "         [-0.8413,  0.2838, -0.8829,  0.6033]],\n",
       "\n",
       "        [[-1.6119, -0.4196,  0.1711, -0.4332],\n",
       "         [ 0.5404,  0.3210, -0.1909,  0.1890],\n",
       "         [-0.3440, -0.4347,  0.4034,  0.4023],\n",
       "         [ 0.3651,  0.3786, -0.3576, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1057, -0.0205, -0.3543],\n",
       "         [-0.1933, -0.7515, -0.6539, -0.2760],\n",
       "         [ 0.1216, -0.7439, -1.1184,  1.5729],\n",
       "         [-0.4201,  0.2262,  0.0809,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.3528,  0.5769,  0.1172],\n",
       "         [-0.2947, -0.3723,  0.3728,  0.3279],\n",
       "         [-0.5404,  1.6815,  0.1997,  0.6438],\n",
       "         [-0.0393, -0.4538,  0.2831, -0.0655]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1204, 0.3005, 0.3256, 0.2534], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = F.softmax((q1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1204, 0.3005, 0.3256, 0.2534],\n",
       "        [0.2323, 0.2047, 0.1567, 0.4063],\n",
       "        [0.2193, 0.2366, 0.1772, 0.3670],\n",
       "        [0.2967, 0.2474, 0.3606, 0.0952]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4755,  0.7773,  2.0798],\n",
       "        [ 0.1360,  0.4734,  0.2271],\n",
       "        [ 0.0423, -0.0171,  1.2723],\n",
       "        [-0.2565,  0.5834,  1.0827]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0676,  0.3781,  1.0074], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 @ v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0676,  0.3781,  1.0074],\n",
       "        [-0.1802,  0.5118,  1.1689],\n",
       "        [-0.1587,  0.4935,  1.1326],\n",
       "        [-0.1166,  0.3971,  1.2352]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5455, -1.0743,  0.2637], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = q[0,1]\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2323, 0.2047, 0.1567, 0.4063], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = F.softmax((q2@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1802,  0.5118,  1.1689], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3518,  0.0418,  0.4753], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq1 = q[1,0]\n",
    "keys = k[1]\n",
    "scores = F.softmax((qq1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores@v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3518,  0.0418,  0.4753],\n",
       "        [-0.4657,  0.1529,  0.5513],\n",
       "        [-0.3086, -0.0077,  0.4688],\n",
       "        [-0.4950,  0.1789,  0.5791]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
