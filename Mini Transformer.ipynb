{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax(querries@keys.permute(0,2,1) \\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "q , k, v, s, c = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 3]),\n",
       " torch.Size([5, 3, 4]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = q[0,0]\n",
    "keys = k[0]\n",
    "values = v[0]\n",
    "ctx_vecs = c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6311,  0.8655, -0.6542,  0.8358], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9810,  0.0888,  1.2204,  0.2591],\n",
       "        [-0.5531, -0.0441,  0.8234,  0.0949],\n",
       "        [ 0.2831,  0.3775, -0.8896,  0.3015]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1142, -0.1486,  0.9820], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2924, 0.2564, 0.4512], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs = F.softmax(q1@keys.T/np.sqrt(4), dim = 0)\n",
    "scrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2924, 0.2564, 0.4512], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2924, 0.2564, 0.4512],\n",
       "         [0.3046, 0.2838, 0.4116],\n",
       "         [0.2343, 0.2961, 0.4696]],\n",
       "\n",
       "        [[0.4613, 0.2667, 0.2720],\n",
       "         [0.4006, 0.2337, 0.3658],\n",
       "         [0.3506, 0.3719, 0.2775]],\n",
       "\n",
       "        [[0.3710, 0.1780, 0.4511],\n",
       "         [0.3469, 0.2786, 0.3746],\n",
       "         [0.3605, 0.3517, 0.2878]],\n",
       "\n",
       "        [[0.5035, 0.3705, 0.1260],\n",
       "         [0.2137, 0.3649, 0.4215],\n",
       "         [0.4223, 0.2059, 0.3717]],\n",
       "\n",
       "        [[0.3654, 0.1255, 0.5091],\n",
       "         [0.3464, 0.3612, 0.2925],\n",
       "         [0.3637, 0.3061, 0.3302]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(q@k.permute(0,2,1) /np.sqrt(4), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0245,  0.2283,  0.1212, -0.0549], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0245,  0.2283,  0.1212, -0.0549],\n",
       "        [-0.0014,  0.2885,  0.2080, -0.1357],\n",
       "        [ 0.0480,  0.1839,  0.0513,  0.0027]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0245,  0.2283,  0.1212, -0.0549],\n",
       "         [-0.0014,  0.2885,  0.2080, -0.1357],\n",
       "         [ 0.0480,  0.1839,  0.0513,  0.0027]],\n",
       "\n",
       "        [[ 0.3760, -0.3855, -0.8556,  0.4556],\n",
       "         [ 0.3390, -0.3638, -0.7825,  0.4329],\n",
       "         [ 0.3513, -0.3098, -0.7639,  0.3911]],\n",
       "\n",
       "        [[-0.0245,  0.0642,  0.0446,  0.1075],\n",
       "         [-0.0181,  0.1347,  0.0906,  0.0428],\n",
       "         [ 0.0565,  0.2402,  0.0605,  0.0106]],\n",
       "\n",
       "        [[-0.0197,  0.5554,  0.4535, -0.4049],\n",
       "         [ 0.0146,  0.2928,  0.2264, -0.3642],\n",
       "         [-0.1527,  0.4471,  0.5880, -0.5725]],\n",
       "\n",
       "        [[ 0.0635,  0.2675,  0.0897,  0.0344],\n",
       "         [ 0.0517,  0.4614,  0.2783, -0.2542],\n",
       "         [ 0.0629,  0.4318,  0.2335, -0.1895]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [self_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = torch.cat([head(x)[4] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "multihead = multi_head_attn(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = multihead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0231, -1.9662,  0.3008, -0.1364, -0.1915,  0.9702],\n",
       "         [ 0.4701,  1.9460, -0.8554, -0.4478, -0.0873, -1.0257],\n",
       "         [ 1.0534, -1.7376, -0.5520,  1.0203, -0.3930,  0.6089]],\n",
       "\n",
       "        [[ 0.5825, -0.5098, -1.1385,  0.1325, -0.8846,  1.8178],\n",
       "         [-1.4548,  1.3502,  0.7119, -1.0193, -0.2577,  0.6696],\n",
       "         [-0.4788,  0.5429, -0.1157, -0.6507, -1.1968,  1.8992]],\n",
       "\n",
       "        [[ 1.2525, -1.1593, -1.4113,  1.0233,  0.0978,  0.1970],\n",
       "         [-0.3135, -0.4095, -1.3153, -0.4397,  1.8473,  0.6308],\n",
       "         [ 1.2602, -0.6129, -1.1350,  0.8786,  0.7792, -1.1700]],\n",
       "\n",
       "        [[ 1.3845, -0.5001,  0.3169,  1.0697, -1.0645, -1.2064],\n",
       "         [-0.1231, -1.0529,  1.7497,  0.0922, -1.2230,  0.5570],\n",
       "         [ 0.8774, -0.9101, -1.7569,  0.6996,  0.8852,  0.2048]],\n",
       "\n",
       "        [[-0.7497,  1.6234,  1.0140, -1.2375, -0.2001, -0.4502],\n",
       "         [-0.7554,  1.3924,  0.1645,  0.8395,  0.0197, -1.6608],\n",
       "         [-1.3110, -0.0224, -0.8433, -0.2656,  1.7294,  0.7128]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(multihead.LNorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  9.3132e-09, -9.9341e-09],\n",
       "        [ 0.0000e+00, -9.9341e-09,  1.9868e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.4835e-09, -1.9868e-08, -2.9802e-08],\n",
       "        [ 1.4901e-08, -5.9605e-08,  1.5895e-07]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.mean(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954]], grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.std(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    '''\n",
    "    The complete encoder module.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, emb_dim, h, parallelize = False, ffn_l1_out_fts = 2048 ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, parallelize)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = self.mul_h_attn(x)\n",
    "        out = torch.relu(self.l1(ctx_vecs))\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return self.LNorm(out + x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "enc = encoder(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (l1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6807, -0.4590,  1.8553, -0.4325, -0.4071, -1.2374],\n",
       "         [ 1.1996,  0.5036, -1.2591, -0.8528,  1.1819, -0.7732],\n",
       "         [ 0.4374, -1.4284, -1.1448,  0.5949,  0.0936,  1.4473]],\n",
       "\n",
       "        [[-0.6799, -0.3918,  0.5790, -1.6221,  1.3587,  0.7562],\n",
       "         [ 0.5610, -1.5596,  0.0128, -1.0685,  1.0122,  1.0421],\n",
       "         [ 0.2855, -0.9815,  1.1326,  0.3345, -1.6628,  0.8918]],\n",
       "\n",
       "        [[-1.1680, -1.2111, -0.1682,  1.6213,  0.2591,  0.6670],\n",
       "         [-1.5072, -1.1199,  0.5933,  1.0595, -0.0252,  0.9994],\n",
       "         [ 1.0912, -0.6212, -1.1809, -0.9823,  0.2850,  1.4082]],\n",
       "\n",
       "        [[ 1.0300,  0.4345, -2.1187,  0.4893,  0.1470,  0.0179],\n",
       "         [-0.2882, -0.8012, -0.0468, -1.2432,  1.8592,  0.5202],\n",
       "         [ 1.1637, -1.4063,  0.8195, -0.6638,  0.9243, -0.8375]],\n",
       "\n",
       "        [[-1.2617,  0.1805,  1.5193, -0.1089, -1.1649,  0.8356],\n",
       "         [ 1.7452, -0.0804, -0.6855,  0.6709, -0.2481, -1.4021],\n",
       "         [-1.6552,  0.5707,  0.2026,  0.8890,  1.0219, -1.0290]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = enc(x)\n",
    "enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder_attention(nn.Module):\n",
    "    '''\n",
    "    Module to implement the encoder_decoder attention layer. \n",
    "    This is same as the self_attention layer except that it takes two input vectors: \n",
    "                 1)encoder's final output \n",
    "                 2) output from previous decoder layer\n",
    "    The querries are generated from the previous decoder layer's output\n",
    "    The keys and the values are generated from the encoder's output \n",
    "         \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Key vector\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Value vector\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = enc_out.shape[0]\n",
    "        seq_len = dec_out.shape[1] \n",
    "        querries = self.WQ(dec_out)\n",
    "        keys = self.WK(enc_out)\n",
    "        values = self.WV(enc_out)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1))\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_decoder_attention(\n",
       "  (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = encoder_decoder_attention(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, s, c = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 4]),\n",
       " torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1055, 1.0679, 0.7241], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = q[0,0]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = k[0]\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2760, -0.6922, -0.5533, -0.9880], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760, -0.6922, -0.5533, -0.9880],\n",
       "         [ 0.0452, -0.1738, -0.6367,  1.0138],\n",
       "         [-0.2108, -0.0797, -0.5803,  0.6809],\n",
       "         [ 0.8407,  0.5261,  1.1788, -1.1274]],\n",
       "\n",
       "        [[-0.0912,  0.8676, -0.7004, -0.8413],\n",
       "         [-0.0349, -0.4209,  0.2272,  0.2838],\n",
       "         [-0.2665,  1.3260, -0.7065, -0.8829],\n",
       "         [-0.1835, -1.0982,  0.4689,  0.6033]],\n",
       "\n",
       "        [[-1.6119,  0.5404, -0.3440,  0.3651],\n",
       "         [-0.4196,  0.3210, -0.4347,  0.3786],\n",
       "         [ 0.1711, -0.1909,  0.4034, -0.3576],\n",
       "         [-0.4332,  0.1890,  0.4023, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1933,  0.1216, -0.4201],\n",
       "         [-0.1057, -0.7515, -0.7439,  0.2262],\n",
       "         [-0.0205, -0.6539, -1.1184,  0.0809],\n",
       "         [-0.3543, -0.2760,  1.5729,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.2947, -0.5404, -0.0393],\n",
       "         [-0.3528, -0.3723,  1.6815, -0.4538],\n",
       "         [ 0.5769,  0.3728,  0.1997,  0.2831],\n",
       "         [ 0.1172,  0.3279,  0.6438, -0.0655]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760,  0.0452, -0.2108,  0.8407],\n",
       "         [-0.6922, -0.1738, -0.0797,  0.5261],\n",
       "         [-0.5533, -0.6367, -0.5803,  1.1788],\n",
       "         [-0.9880,  1.0138,  0.6809, -1.1274]],\n",
       "\n",
       "        [[-0.0912, -0.0349, -0.2665, -0.1835],\n",
       "         [ 0.8676, -0.4209,  1.3260, -1.0982],\n",
       "         [-0.7004,  0.2272, -0.7065,  0.4689],\n",
       "         [-0.8413,  0.2838, -0.8829,  0.6033]],\n",
       "\n",
       "        [[-1.6119, -0.4196,  0.1711, -0.4332],\n",
       "         [ 0.5404,  0.3210, -0.1909,  0.1890],\n",
       "         [-0.3440, -0.4347,  0.4034,  0.4023],\n",
       "         [ 0.3651,  0.3786, -0.3576, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1057, -0.0205, -0.3543],\n",
       "         [-0.1933, -0.7515, -0.6539, -0.2760],\n",
       "         [ 0.1216, -0.7439, -1.1184,  1.5729],\n",
       "         [-0.4201,  0.2262,  0.0809,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.3528,  0.5769,  0.1172],\n",
       "         [-0.2947, -0.3723,  0.3728,  0.3279],\n",
       "         [-0.5404,  1.6815,  0.1997,  0.6438],\n",
       "         [-0.0393, -0.4538,  0.2831, -0.0655]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1204, 0.3005, 0.3256, 0.2534], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = F.softmax((q1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1204, 0.3005, 0.3256, 0.2534],\n",
       "        [0.2323, 0.2047, 0.1567, 0.4063],\n",
       "        [0.2193, 0.2366, 0.1772, 0.3670],\n",
       "        [0.2967, 0.2474, 0.3606, 0.0952]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4755,  0.7773,  2.0798],\n",
       "        [ 0.1360,  0.4734,  0.2271],\n",
       "        [ 0.0423, -0.0171,  1.2723],\n",
       "        [-0.2565,  0.5834,  1.0827]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0676,  0.3781,  1.0074], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 @ v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0676,  0.3781,  1.0074],\n",
       "        [-0.1802,  0.5118,  1.1689],\n",
       "        [-0.1587,  0.4935,  1.1326],\n",
       "        [-0.1166,  0.3971,  1.2352]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5455, -1.0743,  0.2637], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = q[0,1]\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2323, 0.2047, 0.1567, 0.4063], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = F.softmax((q2@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1802,  0.5118,  1.1689], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3518,  0.0418,  0.4753], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq1 = q[1,0]\n",
    "keys = k[1]\n",
    "scores = F.softmax((qq1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores@v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3518,  0.0418,  0.4753],\n",
       "        [-0.4657,  0.1529,  0.5513],\n",
       "        [-0.3086, -0.0077,  0.4688],\n",
       "        [-0.4950,  0.1789,  0.5791]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_enc_dec_attn(nn.Module):\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [encoder_decoder_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        ctx_vecs = torch.cat([head(enc_out, dec_out)[4] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(dec_out + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_enc_dec_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 7])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_enc_dec_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "enc_seq_len = 4\n",
    "dec_seq_len = 2\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, enc_seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, dec_seq_len, emb_dim)\n",
    "enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5825,  0.2314, -0.0871,  1.1878, -0.7354,  1.4921, -0.5063],\n",
       "         [ 2.1371, -0.9590, -0.0456,  0.0407,  0.4044, -0.5633, -1.0142]],\n",
       "\n",
       "        [[-0.8367, -0.5809,  1.1473, -0.4594,  0.9021,  1.2566, -1.4290],\n",
       "         [-0.6285, -0.3225, -0.0123,  0.3943, -1.7571,  1.6886,  0.6375]],\n",
       "\n",
       "        [[-0.6189, -1.2911,  1.2547, -0.8207,  1.6273, -0.2217,  0.0704],\n",
       "         [-0.5710,  0.1586,  0.0643,  2.3108, -0.6349, -0.5634, -0.7644]],\n",
       "\n",
       "        [[ 1.3261, -0.4710,  0.2420,  0.1493,  0.0888, -2.0887,  0.7536],\n",
       "         [ 0.7919, -1.4163, -1.0918,  0.2870, -0.7728,  1.2891,  0.9129]],\n",
       "\n",
       "        [[-0.0049, -0.8477, -0.4687, -1.3792,  1.7102,  1.1053, -0.1150],\n",
       "         [ 0.0845,  1.8900, -0.6647, -0.9882,  1.0307, -0.5630, -0.7893]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4787,  0.1758,  0.0812,  1.3959, -0.9545,  1.2879, -0.5076],\n",
       "         [ 2.0926, -1.0435,  0.2474,  0.3530, -0.0164, -0.7385, -0.8946]],\n",
       "\n",
       "        [[-0.7540, -0.6133,  0.9736, -0.5767,  1.1325,  1.2379, -1.3999],\n",
       "         [-0.6942, -0.3792, -0.1381,  0.2258, -1.5779,  1.7958,  0.7678]],\n",
       "\n",
       "        [[-0.8495, -1.2767,  1.4893, -0.8838,  1.2699,  0.1748,  0.0759],\n",
       "         [-0.6369,  0.3687,  0.0556,  2.1963, -0.8476, -0.2022, -0.9339]],\n",
       "\n",
       "        [[ 1.6768, -0.1492,  0.3126, -0.3615, -0.1140, -1.9057,  0.5410],\n",
       "         [ 1.1211, -1.1274, -1.0408, -0.1686, -0.9393,  1.3565,  0.7985]],\n",
       "\n",
       "        [[ 0.1336, -1.2479, -0.5530, -1.1831,  1.5362,  1.1550,  0.1590],\n",
       "         [ 0.2815,  2.0475, -0.7514, -0.6693,  0.6693, -0.8974, -0.6802]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slfattn = multi_head_attn(emb_dim, h)\n",
    "slfattn(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out2 = torch.randn(batch_size, dec_seq_len+1, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0689,  0.3102, -1.0180, -0.9410, -0.9588,  1.8263,  0.8501],\n",
       "         [-1.1982,  0.4798,  0.4758, -0.9345, -0.0737, -0.6874,  1.9382],\n",
       "         [ 0.0330,  0.3672,  1.9666, -0.3918,  0.0078, -0.3290, -1.6537]],\n",
       "\n",
       "        [[ 0.1886, -0.4317,  1.8125, -1.1055,  1.0329, -0.5438, -0.9529],\n",
       "         [ 0.9974,  1.1519, -1.2553, -1.2514,  0.9556, -0.7704,  0.1721],\n",
       "         [ 0.0976, -0.7475, -1.5488, -0.6047,  0.1723,  1.6123,  1.0188]],\n",
       "\n",
       "        [[ 1.8695, -1.4863, -0.1209, -0.3683, -0.7031,  0.0023,  0.8069],\n",
       "         [ 0.2162,  0.0424,  2.0732, -0.7594, -0.0695, -0.0651, -1.4378],\n",
       "         [-0.3043, -0.2585, -0.8746,  0.3061,  2.2664, -0.2505, -0.8847]],\n",
       "\n",
       "        [[ 0.7157, -1.9892,  0.5818, -0.1268, -0.7911,  0.4477,  1.1619],\n",
       "         [ 1.3221,  1.2725, -1.7518,  0.0332, -0.5668, -0.4657,  0.1566],\n",
       "         [-1.0578,  0.5653, -0.6794, -0.9340, -0.6460,  1.4846,  1.2674]],\n",
       "\n",
       "        [[ 0.7314, -0.5585,  0.9822, -0.5667, -1.3785, -0.7571,  1.5471],\n",
       "         [-0.1461, -1.9451,  0.0865,  1.2358,  0.2069,  1.1354, -0.5734],\n",
       "         [-0.2100,  0.9028,  1.7922,  0.1943, -0.9649, -0.3621, -1.3523]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slfattn(dec_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    '''\n",
    "    The complete decoder module. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, parallelize = False, ffn_l1_out_fts = 2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, parallelize)\n",
    "        \n",
    "        # multi head encoder decoder attention sublayer\n",
    "        self.mul_h_enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "    def forward(self, enc_vecs, dec_vecs):\n",
    "        dec_vecs = self.mul_h_attn(dec_vecs)\n",
    "        ff_in = self.mul_h_enc_dec_attn(enc_vecs, dec_vecs)\n",
    "        out = torch.relu(self.l1(ff_in))\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return self.LNorm(out + ff_in)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "    (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "    (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "    (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (l1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "enc_seq_len = 4\n",
    "dec_seq_len = 2\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, enc_seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, dec_seq_len, emb_dim)\n",
    "dec = decoder(emb_dim, h)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 7])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ot = dec(enc_out, dec_out)\n",
    "dec_ot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 7])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
