{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax(querries@keys.permute(0,2,1) \\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "q , k, v, s, c = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 3]),\n",
       " torch.Size([5, 3, 4]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = q[0,0]\n",
    "keys = k[0]\n",
    "values = v[0]\n",
    "ctx_vecs = c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6311,  0.8655, -0.6542,  0.8358], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9810,  0.0888,  1.2204,  0.2591],\n",
       "        [-0.5531, -0.0441,  0.8234,  0.0949],\n",
       "        [ 0.2831,  0.3775, -0.8896,  0.3015]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1142, -0.1486,  0.9820], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2924, 0.2564, 0.4512], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs = F.softmax(q1@keys.T/np.sqrt(4), dim = 0)\n",
    "scrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2924, 0.2564, 0.4512], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2924, 0.2564, 0.4512],\n",
       "         [0.3046, 0.2838, 0.4116],\n",
       "         [0.2343, 0.2961, 0.4696]],\n",
       "\n",
       "        [[0.4613, 0.2667, 0.2720],\n",
       "         [0.4006, 0.2337, 0.3658],\n",
       "         [0.3506, 0.3719, 0.2775]],\n",
       "\n",
       "        [[0.3710, 0.1780, 0.4511],\n",
       "         [0.3469, 0.2786, 0.3746],\n",
       "         [0.3605, 0.3517, 0.2878]],\n",
       "\n",
       "        [[0.5035, 0.3705, 0.1260],\n",
       "         [0.2137, 0.3649, 0.4215],\n",
       "         [0.4223, 0.2059, 0.3717]],\n",
       "\n",
       "        [[0.3654, 0.1255, 0.5091],\n",
       "         [0.3464, 0.3612, 0.2925],\n",
       "         [0.3637, 0.3061, 0.3302]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(q@k.permute(0,2,1) /np.sqrt(4), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0245,  0.2283,  0.1212, -0.0549], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0245,  0.2283,  0.1212, -0.0549],\n",
       "        [-0.0014,  0.2885,  0.2080, -0.1357],\n",
       "        [ 0.0480,  0.1839,  0.0513,  0.0027]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0245,  0.2283,  0.1212, -0.0549],\n",
       "         [-0.0014,  0.2885,  0.2080, -0.1357],\n",
       "         [ 0.0480,  0.1839,  0.0513,  0.0027]],\n",
       "\n",
       "        [[ 0.3760, -0.3855, -0.8556,  0.4556],\n",
       "         [ 0.3390, -0.3638, -0.7825,  0.4329],\n",
       "         [ 0.3513, -0.3098, -0.7639,  0.3911]],\n",
       "\n",
       "        [[-0.0245,  0.0642,  0.0446,  0.1075],\n",
       "         [-0.0181,  0.1347,  0.0906,  0.0428],\n",
       "         [ 0.0565,  0.2402,  0.0605,  0.0106]],\n",
       "\n",
       "        [[-0.0197,  0.5554,  0.4535, -0.4049],\n",
       "         [ 0.0146,  0.2928,  0.2264, -0.3642],\n",
       "         [-0.1527,  0.4471,  0.5880, -0.5725]],\n",
       "\n",
       "        [[ 0.0635,  0.2675,  0.0897,  0.0344],\n",
       "         [ 0.0517,  0.4614,  0.2783, -0.2542],\n",
       "         [ 0.0629,  0.4318,  0.2335, -0.1895]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [self_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = torch.cat([head(x)[3] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "multihead = multi_head_attn(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = multihead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3970, -0.2556, -0.4025, -1.2867,  0.3908,  1.9510],\n",
       "         [-1.0594,  0.7616,  1.2213, -1.5387,  0.6606, -0.0454],\n",
       "         [-1.9424,  0.8693, -0.0473, -0.4243,  0.9915,  0.5532]],\n",
       "\n",
       "        [[ 0.3326,  0.4952, -2.1718,  0.8860,  0.0920,  0.3660],\n",
       "         [ 0.9578, -1.8795, -0.5415, -0.0991,  0.8968,  0.6655],\n",
       "         [-0.9096, -1.4197,  0.7999,  1.3254,  0.7101, -0.5061]],\n",
       "\n",
       "        [[-0.1232,  0.9751,  0.1593,  0.8051,  0.2561, -2.0724],\n",
       "         [-0.1782, -0.4761,  0.8225,  0.6757,  1.0362, -1.8801],\n",
       "         [-0.8773,  0.5034, -0.4839, -0.2464, -0.8748,  1.9791]],\n",
       "\n",
       "        [[ 0.1125, -1.9037,  0.3175, -0.1553,  1.4896,  0.1396],\n",
       "         [-1.5095,  0.8640, -1.1197, -0.0211,  0.6430,  1.1433],\n",
       "         [-1.2516,  0.8050,  0.4536, -1.5385,  0.9071,  0.6243]],\n",
       "\n",
       "        [[ 0.7306, -1.5146, -1.0940,  0.0747,  1.3162,  0.4870],\n",
       "         [ 1.7558,  0.5619, -0.2374, -0.4283, -0.1200, -1.5320],\n",
       "         [-0.0811,  0.6744,  1.7611, -1.0020, -0.1673, -1.1851]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(multihead.LNorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  9.3132e-09, -9.9341e-09],\n",
       "        [ 0.0000e+00, -9.9341e-09,  1.9868e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.4835e-09, -1.9868e-08, -2.9802e-08],\n",
       "        [ 1.4901e-08, -5.9605e-08,  1.5895e-07]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.mean(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954]], grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.std(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    '''\n",
    "    The complete encoder module.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, emb_dim, h, parallelize = False, ffn_l1_out_fts = 2048 ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, parallelize)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = self.mul_h_attn(x)\n",
    "        out = torch.relu(self.l1(ctx_vecs))\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return self.LNorm(out + x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "enc = encoder(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (l1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6807, -0.4590,  1.8553, -0.4325, -0.4071, -1.2374],\n",
       "         [ 1.1996,  0.5036, -1.2591, -0.8528,  1.1819, -0.7732],\n",
       "         [ 0.4374, -1.4284, -1.1448,  0.5949,  0.0936,  1.4473]],\n",
       "\n",
       "        [[-0.6799, -0.3918,  0.5790, -1.6221,  1.3587,  0.7562],\n",
       "         [ 0.5610, -1.5596,  0.0128, -1.0685,  1.0122,  1.0421],\n",
       "         [ 0.2855, -0.9815,  1.1326,  0.3345, -1.6628,  0.8918]],\n",
       "\n",
       "        [[-1.1680, -1.2111, -0.1682,  1.6213,  0.2591,  0.6670],\n",
       "         [-1.5072, -1.1199,  0.5933,  1.0595, -0.0252,  0.9994],\n",
       "         [ 1.0912, -0.6212, -1.1809, -0.9823,  0.2850,  1.4082]],\n",
       "\n",
       "        [[ 1.0300,  0.4345, -2.1187,  0.4893,  0.1470,  0.0179],\n",
       "         [-0.2882, -0.8012, -0.0468, -1.2432,  1.8592,  0.5202],\n",
       "         [ 1.1637, -1.4063,  0.8195, -0.6638,  0.9243, -0.8375]],\n",
       "\n",
       "        [[-1.2617,  0.1805,  1.5193, -0.1089, -1.1649,  0.8356],\n",
       "         [ 1.7452, -0.0804, -0.6855,  0.6709, -0.2481, -1.4021],\n",
       "         [-1.6552,  0.5707,  0.2026,  0.8890,  1.0219, -1.0290]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = enc(x)\n",
    "enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder_attention(nn.Module):\n",
    "    '''\n",
    "    Module to implement the encoder_decoder attention layer. \n",
    "    This is same as the self_attention layer except that it takes two input vectors: \n",
    "                 1)encoder's final output \n",
    "                 2) output from previous decoder layer\n",
    "    The querries are generated from the previous decoder layer's output\n",
    "    The keys and the values are generated from the encoder's output \n",
    "         \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Key vector\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Value vector\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = enc_out.shape[0]\n",
    "        seq_len = enc_out.shape[1]\n",
    "        querries = self.WQ(dec_out)\n",
    "        keys = self.WK(enc_out)\n",
    "        values = self.WV(enc_out)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1))\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_decoder_attention(\n",
       "  (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = encoder_decoder_attention(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, s, c = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 4]),\n",
       " torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1055, 1.0679, 0.7241], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = q[0,0]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = k[0]\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2760, -0.6922, -0.5533, -0.9880], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760, -0.6922, -0.5533, -0.9880],\n",
       "         [ 0.0452, -0.1738, -0.6367,  1.0138],\n",
       "         [-0.2108, -0.0797, -0.5803,  0.6809],\n",
       "         [ 0.8407,  0.5261,  1.1788, -1.1274]],\n",
       "\n",
       "        [[-0.0912,  0.8676, -0.7004, -0.8413],\n",
       "         [-0.0349, -0.4209,  0.2272,  0.2838],\n",
       "         [-0.2665,  1.3260, -0.7065, -0.8829],\n",
       "         [-0.1835, -1.0982,  0.4689,  0.6033]],\n",
       "\n",
       "        [[-1.6119,  0.5404, -0.3440,  0.3651],\n",
       "         [-0.4196,  0.3210, -0.4347,  0.3786],\n",
       "         [ 0.1711, -0.1909,  0.4034, -0.3576],\n",
       "         [-0.4332,  0.1890,  0.4023, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1933,  0.1216, -0.4201],\n",
       "         [-0.1057, -0.7515, -0.7439,  0.2262],\n",
       "         [-0.0205, -0.6539, -1.1184,  0.0809],\n",
       "         [-0.3543, -0.2760,  1.5729,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.2947, -0.5404, -0.0393],\n",
       "         [-0.3528, -0.3723,  1.6815, -0.4538],\n",
       "         [ 0.5769,  0.3728,  0.1997,  0.2831],\n",
       "         [ 0.1172,  0.3279,  0.6438, -0.0655]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2760,  0.0452, -0.2108,  0.8407],\n",
       "         [-0.6922, -0.1738, -0.0797,  0.5261],\n",
       "         [-0.5533, -0.6367, -0.5803,  1.1788],\n",
       "         [-0.9880,  1.0138,  0.6809, -1.1274]],\n",
       "\n",
       "        [[-0.0912, -0.0349, -0.2665, -0.1835],\n",
       "         [ 0.8676, -0.4209,  1.3260, -1.0982],\n",
       "         [-0.7004,  0.2272, -0.7065,  0.4689],\n",
       "         [-0.8413,  0.2838, -0.8829,  0.6033]],\n",
       "\n",
       "        [[-1.6119, -0.4196,  0.1711, -0.4332],\n",
       "         [ 0.5404,  0.3210, -0.1909,  0.1890],\n",
       "         [-0.3440, -0.4347,  0.4034,  0.4023],\n",
       "         [ 0.3651,  0.3786, -0.3576, -0.3989]],\n",
       "\n",
       "        [[-0.1977, -0.1057, -0.0205, -0.3543],\n",
       "         [-0.1933, -0.7515, -0.6539, -0.2760],\n",
       "         [ 0.1216, -0.7439, -1.1184,  1.5729],\n",
       "         [-0.4201,  0.2262,  0.0809,  0.3774]],\n",
       "\n",
       "        [[-0.2560, -0.3528,  0.5769,  0.1172],\n",
       "         [-0.2947, -0.3723,  0.3728,  0.3279],\n",
       "         [-0.5404,  1.6815,  0.1997,  0.6438],\n",
       "         [-0.0393, -0.4538,  0.2831, -0.0655]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1204, 0.3005, 0.3256, 0.2534], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = F.softmax((q1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1204, 0.3005, 0.3256, 0.2534],\n",
       "        [0.2323, 0.2047, 0.1567, 0.4063],\n",
       "        [0.2193, 0.2366, 0.1772, 0.3670],\n",
       "        [0.2967, 0.2474, 0.3606, 0.0952]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4755,  0.7773,  2.0798],\n",
       "        [ 0.1360,  0.4734,  0.2271],\n",
       "        [ 0.0423, -0.0171,  1.2723],\n",
       "        [-0.2565,  0.5834,  1.0827]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0676,  0.3781,  1.0074], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 @ v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0676,  0.3781,  1.0074],\n",
       "        [-0.1802,  0.5118,  1.1689],\n",
       "        [-0.1587,  0.4935,  1.1326],\n",
       "        [-0.1166,  0.3971,  1.2352]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5455, -1.0743,  0.2637], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = q[0,1]\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2323, 0.2047, 0.1567, 0.4063], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = F.softmax((q2@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1802,  0.5118,  1.1689], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3518,  0.0418,  0.4753], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq1 = q[1,0]\n",
    "keys = k[1]\n",
    "scores = F.softmax((qq1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores@v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3518,  0.0418,  0.4753],\n",
       "        [-0.4657,  0.1529,  0.5513],\n",
       "        [-0.3086, -0.0077,  0.4688],\n",
       "        [-0.4950,  0.1789,  0.5791]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_enc_dec_attn(nn.Module):\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = [encoder_decoder_attention(emb_dim, h) for i in range(h)]\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        ctx_vecs = torch.cat([head(enc_out, dec_out)[4] for head in self.heads], dim = 2)\n",
    "        transformed = self.Wo(ctx_vecs)\n",
    "        \n",
    "        return self.LNorm(dec_out + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_enc_dec_attn(\n",
       "  (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 7])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
