{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1)).permute(0,2,1), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 512\n",
    "h = 8\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=512, out_features=64, bias=False)\n",
       "  (WK): Linear(in_features=512, out_features=64, bias=False)\n",
       "  (WV): Linear(in_features=512, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "querries, keys, values, att_scores, ctx_vecs = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 64]),\n",
       " torch.Size([5, 3, 64]),\n",
       " torch.Size([5, 3, 64]),\n",
       " torch.Size([5, 3, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querries.shape, keys.shape, values.shape, ctx_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.7062e-03, 8.0831e-01, 1.8499e-01],\n",
       "         [5.6242e-02, 1.6197e-01, 7.8178e-01],\n",
       "         [9.7505e-01, 4.1655e-03, 2.0783e-02]],\n",
       "\n",
       "        [[3.1738e-02, 8.8465e-01, 8.3607e-02],\n",
       "         [8.6128e-01, 6.6878e-02, 7.1839e-02],\n",
       "         [8.9534e-01, 4.2649e-02, 6.2008e-02]],\n",
       "\n",
       "        [[4.3472e-02, 8.5899e-01, 9.7543e-02],\n",
       "         [1.5764e-03, 1.4066e-02, 9.8436e-01],\n",
       "         [5.0195e-01, 1.5746e-02, 4.8231e-01]],\n",
       "\n",
       "        [[9.9267e-01, 1.2288e-03, 6.1012e-03],\n",
       "         [3.0698e-02, 5.0208e-03, 9.6428e-01],\n",
       "         [5.6540e-01, 1.0565e-01, 3.2894e-01]],\n",
       "\n",
       "        [[9.5420e-02, 8.9621e-01, 8.3741e-03],\n",
       "         [2.1166e-02, 2.6959e-01, 7.0924e-01],\n",
       "         [1.6179e-03, 9.9821e-01, 1.6967e-04]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1312, -0.1994, -0.8424, -0.2728, -0.1895, -0.4478, -0.2879,  0.1850,\n",
       "         0.0476, -0.2647, -0.1008,  1.0653, -0.7457, -0.9336,  0.0162,  0.3052,\n",
       "         0.1973,  0.1181,  0.4215, -0.4157, -0.7890,  0.1855,  0.7934, -0.1824,\n",
       "        -0.0045,  0.2831,  0.3987, -0.4449,  0.7939,  0.0970,  0.8292, -0.3705,\n",
       "        -0.2376,  0.1030,  0.1147,  0.3346,  0.1519, -0.6783,  0.3524, -0.3001,\n",
       "        -0.1445, -0.1007,  0.0783, -0.3000,  0.1789,  0.1023,  0.8283, -0.3681,\n",
       "        -0.2386, -0.1363,  0.4252, -0.3139,  0.2909,  0.8021,  0.4274, -1.2823,\n",
       "         0.4824, -0.0983, -0.6733, -0.4979, -0.9194, -0.4062,  0.4497, -0.0444],\n",
       "       grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores[0,0]@values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores[0,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
