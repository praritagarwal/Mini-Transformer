{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        att_scores = F.softmax(querries@keys.permute(0,2,1) \\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q , k, v, s, c = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 4]),\n",
       " torch.Size([5, 3, 3]),\n",
       " torch.Size([5, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = q[0,0]\n",
    "keys = k[0]\n",
    "values = v[0]\n",
    "ctx_vecs = c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1707, -0.5051, -0.0960, -1.0003], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1833,  0.0102, -0.2582, -0.5697],\n",
       "        [-0.2226,  0.1066, -0.0829,  0.1139],\n",
       "        [-0.6665,  1.0764,  0.5193,  1.2742]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6208, -0.1218, -1.7544], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5013, 0.3458, 0.1529], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs = F.softmax(q1@keys.T/np.sqrt(4), dim = 0)\n",
    "scrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5013, 0.3458, 0.1529], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5013, 0.3458, 0.1529],\n",
       "         [0.2956, 0.3199, 0.3845],\n",
       "         [0.1782, 0.2625, 0.5593]],\n",
       "\n",
       "        [[0.2920, 0.2894, 0.4186],\n",
       "         [0.2995, 0.2933, 0.4073],\n",
       "         [0.3242, 0.3104, 0.3654]],\n",
       "\n",
       "        [[0.4895, 0.3361, 0.1745],\n",
       "         [0.3588, 0.3449, 0.2964],\n",
       "         [0.3035, 0.3204, 0.3761]],\n",
       "\n",
       "        [[0.3343, 0.3388, 0.3269],\n",
       "         [0.3319, 0.3284, 0.3398],\n",
       "         [0.2968, 0.3019, 0.4013]],\n",
       "\n",
       "        [[0.3792, 0.3482, 0.2726],\n",
       "         [0.3807, 0.3489, 0.2704],\n",
       "         [0.2963, 0.3194, 0.3843]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(q@k.permute(0,2,1) /np.sqrt(4), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2465,  0.7646, -0.5022, -0.4824], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrs@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2465,  0.7646, -0.5022, -0.4824],\n",
       "        [ 0.1580,  0.6186, -0.2870, -0.5971],\n",
       "        [ 0.4415,  0.5388, -0.1455, -0.6847]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2465,  0.7646, -0.5022, -0.4824],\n",
       "         [ 0.1580,  0.6186, -0.2870, -0.5971],\n",
       "         [ 0.4415,  0.5388, -0.1455, -0.6847]],\n",
       "\n",
       "        [[-0.5347,  0.0648, -0.2954,  0.1957],\n",
       "         [-0.5216,  0.0817, -0.2946,  0.1825],\n",
       "         [-0.4736,  0.1461, -0.2931,  0.1324]],\n",
       "\n",
       "        [[-0.5657,  0.4318, -0.1775,  0.0313],\n",
       "         [-0.4352,  0.2696, -0.0836,  0.0570],\n",
       "         [-0.3721,  0.1683, -0.0400,  0.0858]],\n",
       "\n",
       "        [[ 0.2293,  0.2412,  0.1840, -0.1002],\n",
       "         [ 0.2351,  0.2566,  0.1891, -0.1087],\n",
       "         [ 0.2405,  0.3423,  0.1869, -0.1555]],\n",
       "\n",
       "        [[ 0.4255,  0.0221,  0.3283, -0.1739],\n",
       "         [ 0.4269,  0.0222,  0.3295, -0.1761],\n",
       "         [ 0.3557,  0.0164,  0.2712, -0.0675]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = nn.ModuleList([self_attention(emb_dim, h) for i in range(h)])\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = torch.cat([head(x)[4] for head in self.heads], dim = 2)\n",
    "        transformed = self.drop(self.Wo(ctx_vecs))\n",
    "        \n",
    "        return self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "multihead = multi_head_attn(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = multihead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9259, -1.6872, -0.3940, -0.6162,  1.1956,  0.5760],\n",
       "         [-0.4495, -0.1315, -0.0601,  1.8545,  0.2882, -1.5016],\n",
       "         [-0.3958,  0.9557,  1.5756, -1.3655, -0.7634, -0.0067]],\n",
       "\n",
       "        [[-1.0366,  0.8172,  1.7425, -0.8790,  0.0251, -0.6692],\n",
       "         [-0.8655, -1.3596,  0.4337, -0.0344,  1.7922,  0.0335],\n",
       "         [ 1.2980,  0.8864,  0.1400, -1.0339, -1.5419,  0.2514]],\n",
       "\n",
       "        [[-0.3868, -1.8232,  0.0511,  0.5795,  0.1039,  1.4755],\n",
       "         [-0.4526, -0.0416,  0.3212,  0.2614, -1.7202,  1.6318],\n",
       "         [ 0.1027, -1.3862, -1.2575,  0.3761,  1.0516,  1.1132]],\n",
       "\n",
       "        [[ 0.6206,  0.2592,  1.6546, -1.3964, -0.8949, -0.2430],\n",
       "         [-0.5476,  1.1051, -0.9346, -0.6031,  1.6651, -0.6849],\n",
       "         [ 1.1760, -0.7325, -1.2813, -0.4193, -0.2296,  1.4866]],\n",
       "\n",
       "        [[-1.7762,  0.0581, -0.1710,  1.3911, -0.3646,  0.8627],\n",
       "         [ 0.8566, -0.4226, -0.9785, -1.3076,  1.5198,  0.3323],\n",
       "         [-1.8776, -0.3418, -0.3376,  0.6804,  1.0382,  0.8384]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_attn(\n",
       "  (heads): ModuleList(\n",
       "    (0): self_attention(\n",
       "      (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       "    )\n",
       "    (1): self_attention(\n",
       "      (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(multihead.LNorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9341e-09,  0.0000e+00, -2.9958e-08],\n",
       "        [-9.9341e-09, -1.6143e-08,  1.9868e-08],\n",
       "        [ 1.9868e-08,  1.9868e-08,  0.0000e+00],\n",
       "        [-1.1921e-07,  0.0000e+00,  0.0000e+00],\n",
       "        [-9.9341e-09,  6.4572e-08, -2.9802e-08]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.mean(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954],\n",
       "        [1.0954, 1.0954, 1.0954]], grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.std(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    '''\n",
    "    The complete encoder module.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = False, ffn_l1_out_fts = 2048 ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, p_drop, parallelize)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ctx_vecs = self.mul_h_attn(x)\n",
    "        out = torch.relu(self.l1(ctx_vecs))\n",
    "        out = self.drop(self.l2(out))\n",
    "        \n",
    "        return self.LNorm(out + ctx_vecs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "enc = encoder(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (heads): ModuleList(\n",
       "      (0): self_attention(\n",
       "        (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       "      )\n",
       "      (1): self_attention(\n",
       "        (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (Wo): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (l1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "  (LNorm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0356, -1.7897,  0.4119, -0.1859, -0.0103,  1.6097],\n",
       "         [-0.7671,  1.1825, -0.5971,  1.5617, -0.3256, -1.0544],\n",
       "         [-0.1737,  0.4675, -0.8980, -1.1823,  1.8810, -0.0945]],\n",
       "\n",
       "        [[-0.4613,  0.6158, -0.9576, -0.8353, -0.2879,  1.9262],\n",
       "         [ 0.1543,  1.2555, -0.9611, -1.4111,  1.1960, -0.2336],\n",
       "         [ 0.7304, -1.9692, -0.1333, -0.1860,  0.3776,  1.1806]],\n",
       "\n",
       "        [[-0.1852, -1.9472, -0.0061,  1.3705,  0.4037,  0.3642],\n",
       "         [ 1.4038, -1.3256, -1.0521, -0.2566,  1.0290,  0.2016],\n",
       "         [-0.7487, -1.1383,  0.4902,  1.0123,  1.3765, -0.9919]],\n",
       "\n",
       "        [[-0.4629, -0.7583,  0.1781,  0.8369,  1.5960, -1.3897],\n",
       "         [-0.2909,  1.1685,  1.0087,  0.4790, -0.6797, -1.6856],\n",
       "         [-0.3784,  1.6234, -0.1695,  0.2433,  0.4044, -1.7233]],\n",
       "\n",
       "        [[ 0.6560, -1.5413,  1.5059, -0.7203,  0.4984, -0.3988],\n",
       "         [-0.3705, -0.5636, -1.5141,  1.0945, -0.0776,  1.4313],\n",
       "         [ 0.8952, -0.5541, -0.8983,  1.7043, -0.0628, -1.0844]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = enc(x)\n",
    "enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder_attention(nn.Module):\n",
    "    '''\n",
    "    Module to implement the encoder_decoder attention layer. \n",
    "    This is same as the self_attention layer except that it takes two input vectors: \n",
    "                 1)encoder's final output \n",
    "                 2) output from previous decoder layer\n",
    "    The querries are generated from the previous decoder layer's output\n",
    "    The keys and the values are generated from the encoder's output \n",
    "         \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Key vector\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        # Value vector\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = enc_out.shape[0]\n",
    "        seq_len = dec_out.shape[1] \n",
    "        querries = self.WQ(dec_out)\n",
    "        keys = self.WK(enc_out)\n",
    "        values = self.WV(enc_out)\n",
    "        att_scores = F.softmax((querries@keys.permute(0,2,1))\\\n",
    "                               /np.sqrt(self.red_vec_size), dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_decoder_attention(\n",
       "  (WQ): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WK): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (WV): Linear(in_features=6, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = encoder_decoder_attention(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, s, c = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 3]),\n",
       " torch.Size([5, 4, 4]),\n",
       " torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape, s.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5851, 0.3228, 0.0287], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = q[0,0]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = k[0]\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2386,  0.4622, -0.2803,  0.3015], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2386,  0.4622, -0.2803,  0.3015],\n",
       "         [-0.1076,  0.5195, -0.4099,  0.0666],\n",
       "         [-0.0920,  0.2338, -0.1573,  0.1117],\n",
       "         [-0.0360, -0.1129,  0.1227,  0.0777]],\n",
       "\n",
       "        [[ 0.1965, -0.7966,  0.9295,  0.5020],\n",
       "         [ 0.0727, -0.4285, -0.2303,  0.2997],\n",
       "         [ 0.0424, -0.6974, -0.1429,  0.4903],\n",
       "         [ 0.2310,  0.9476,  0.6705, -0.7224]],\n",
       "\n",
       "        [[ 0.0845, -0.1833, -0.1021, -0.5735],\n",
       "         [ 0.6533,  0.5061,  0.1724,  0.1860],\n",
       "         [-0.4343, -1.2003, -0.6771, -1.7024],\n",
       "         [-0.2764,  0.3819,  0.3625,  0.8314]],\n",
       "\n",
       "        [[ 0.3854,  0.4597,  0.0902,  0.3203],\n",
       "         [-0.3616, -0.5499, -0.4439, -0.3563],\n",
       "         [-0.8763, -1.1128, -0.0414, -1.0954],\n",
       "         [ 0.5250,  0.7198, -0.1168,  0.9565]],\n",
       "\n",
       "        [[-0.0465,  0.0854,  0.4184,  0.5191],\n",
       "         [-0.0551,  0.1738, -0.0035,  0.0927],\n",
       "         [ 0.1029,  0.1281, -0.2238, -0.3167],\n",
       "         [-0.0845,  0.0778, -0.0731,  0.0251]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2386, -0.1076, -0.0920, -0.0360],\n",
       "         [ 0.4622,  0.5195,  0.2338, -0.1129],\n",
       "         [-0.2803, -0.4099, -0.1573,  0.1227],\n",
       "         [ 0.3015,  0.0666,  0.1117,  0.0777]],\n",
       "\n",
       "        [[ 0.1965,  0.0727,  0.0424,  0.2310],\n",
       "         [-0.7966, -0.4285, -0.6974,  0.9476],\n",
       "         [ 0.9295, -0.2303, -0.1429,  0.6705],\n",
       "         [ 0.5020,  0.2997,  0.4903, -0.7224]],\n",
       "\n",
       "        [[ 0.0845,  0.6533, -0.4343, -0.2764],\n",
       "         [-0.1833,  0.5061, -1.2003,  0.3819],\n",
       "         [-0.1021,  0.1724, -0.6771,  0.3625],\n",
       "         [-0.5735,  0.1860, -1.7024,  0.8314]],\n",
       "\n",
       "        [[ 0.3854, -0.3616, -0.8763,  0.5250],\n",
       "         [ 0.4597, -0.5499, -1.1128,  0.7198],\n",
       "         [ 0.0902, -0.4439, -0.0414, -0.1168],\n",
       "         [ 0.3203, -0.3563, -1.0954,  0.9565]],\n",
       "\n",
       "        [[-0.0465, -0.0551,  0.1029, -0.0845],\n",
       "         [ 0.0854,  0.1738,  0.1281,  0.0778],\n",
       "         [ 0.4184, -0.0035, -0.2238, -0.0731],\n",
       "         [ 0.5191,  0.0927, -0.3167,  0.0251]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2066, 0.3096, 0.2017, 0.2822], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = F.softmax((q1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2066, 0.3096, 0.2017, 0.2822],\n",
       "        [0.2282, 0.3278, 0.1917, 0.2524],\n",
       "        [0.2328, 0.2810, 0.2242, 0.2619],\n",
       "        [0.2427, 0.2322, 0.2660, 0.2592]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6055, -0.5224,  0.3504],\n",
       "        [-0.4461,  0.1327,  1.0524],\n",
       "        [ 0.5984, -0.3364, -0.2927],\n",
       "        [ 0.0488,  0.3979, -0.1701]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1214, -0.0224,  0.2912], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 @ v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1214, -0.0224,  0.2912],\n",
       "        [ 0.1190, -0.0398,  0.3259],\n",
       "        [ 0.1626, -0.0556,  0.2672],\n",
       "        [ 0.2152, -0.0823,  0.2075]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1333,  0.0262, -0.5583], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = q[0,1]\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2282, 0.3278, 0.1917, 0.2524], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = F.softmax((q2@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1190, -0.0398,  0.3259], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1126, -0.1594,  0.1096], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq1 = q[1,0]\n",
    "keys = k[1]\n",
    "scores = F.softmax((qq1@keys.T/np.sqrt(3)), dim = 0)\n",
    "scores@v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258e-01, -1.5942e-01,  1.0960e-01],\n",
       "        [ 1.8386e-03, -8.1636e-02,  1.3021e-01],\n",
       "        [ 6.1147e-05, -9.0170e-02,  1.7935e-01],\n",
       "        [-7.9225e-02, -1.1541e-01, -1.7236e-01]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_enc_dec_attn(nn.Module):\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = nn.ModuleList([encoder_decoder_attention(emb_dim, h) for i in range(h)])\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, enc_out, dec_out):\n",
    "        ctx_vecs = torch.cat([head(enc_out, dec_out)[4] for head in self.heads], dim = 2)\n",
    "        transformed = self.drop(self.Wo(ctx_vecs))\n",
    "        \n",
    "        return self.LNorm(dec_out + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_enc_dec_attn(\n",
       "  (heads): ModuleList(\n",
       "    (0): encoder_decoder_attention(\n",
       "      (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "    )\n",
       "    (1): encoder_decoder_attention(\n",
       "      (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 4\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, seq_len, emb_dim)\n",
    "enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 7])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_head_enc_dec_attn(\n",
       "  (heads): ModuleList(\n",
       "    (0): encoder_decoder_attention(\n",
       "      (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "    )\n",
       "    (1): encoder_decoder_attention(\n",
       "      (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "      (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "enc_seq_len = 4\n",
    "dec_seq_len = 2\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, enc_seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, dec_seq_len, emb_dim)\n",
    "enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h)\n",
    "enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3173, -1.6642,  1.7774, -0.3332,  0.4479, -0.5276,  0.6169],\n",
       "         [ 0.1208, -1.3695, -0.9401,  0.0946,  0.5969,  1.9191, -0.4219]],\n",
       "\n",
       "        [[-0.9383,  0.1214,  0.8312,  0.3525,  1.1789,  0.3903, -1.9359],\n",
       "         [-0.5820,  0.9302,  1.2656, -0.7181,  0.8239, -1.7319,  0.0124]],\n",
       "\n",
       "        [[-0.8074,  1.4518, -0.2936,  0.2333,  1.2601, -0.2854, -1.5589],\n",
       "         [ 1.0341,  0.6134, -0.3227, -0.3150, -2.1528,  0.7492,  0.3939]],\n",
       "\n",
       "        [[ 0.5503,  0.9743,  0.7737, -0.5613, -1.6007, -1.1318,  0.9955],\n",
       "         [ 0.8599,  0.2042,  1.9179, -0.6513, -1.1056, -0.8804, -0.3447]],\n",
       "\n",
       "        [[ 1.5442, -0.9471, -0.1551,  1.4458, -0.6907, -1.0515, -0.1455],\n",
       "         [ 0.2330, -0.4681,  1.0447,  1.1513,  0.5465, -1.9123, -0.5951]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_attn(enc_out, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4988, -1.7826,  1.5387,  0.0354,  0.7932, -0.5776,  0.4918],\n",
       "         [-0.0953, -1.0355, -1.2734,  0.5354,  0.7089,  1.7706, -0.6107]],\n",
       "\n",
       "        [[-0.9999,  0.4062,  0.5202,  0.2215,  1.3127,  0.4370, -1.8977],\n",
       "         [-0.6400,  1.0514,  0.9517, -0.5565,  1.1456, -1.7012, -0.2510]],\n",
       "\n",
       "        [[-0.5464,  1.5039, -0.5959,  0.0448,  1.3711, -0.3305, -1.4469],\n",
       "         [ 1.1695,  0.6443, -0.4210, -0.4066, -2.0705,  0.5433,  0.5409]],\n",
       "\n",
       "        [[ 0.2423,  0.5006,  1.3882, -0.3299, -1.5980, -1.1215,  0.9183],\n",
       "         [ 0.2305, -0.4451,  2.2930, -0.2870, -1.0564, -0.4733, -0.2617]],\n",
       "\n",
       "        [[ 0.9524, -1.4725,  0.0829,  1.7462, -0.2753, -0.8763, -0.1575],\n",
       "         [ 0.1144, -1.0332,  0.8611,  1.5092,  0.5765, -1.5197, -0.5082]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slfattn = multi_head_attn(emb_dim, h)\n",
    "slfattn(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out2 = torch.randn(batch_size, dec_seq_len+1, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1029,  0.7477, -1.0303, -1.5965,  1.6525,  0.2635, -0.1398],\n",
       "         [ 1.1281,  1.3965,  0.2882, -1.3040,  0.1451, -1.3769, -0.2771],\n",
       "         [-0.7297, -0.4891,  1.0553,  1.0451, -1.7745,  0.9337, -0.0409]],\n",
       "\n",
       "        [[ 0.2068,  1.2343, -2.1818,  0.7834, -0.0167, -0.1848,  0.1589],\n",
       "         [-0.0838,  0.7375, -1.8564,  0.3816, -1.0447,  1.0443,  0.8216],\n",
       "         [-1.4907, -0.0035,  0.7861, -0.5105, -0.9738,  1.6195,  0.5728]],\n",
       "\n",
       "        [[-0.4152, -0.2604,  0.0931,  1.1401,  1.0777,  0.3974, -2.0327],\n",
       "         [ 0.7210, -0.9723, -1.6990, -0.0964,  1.4246,  0.7668, -0.1447],\n",
       "         [-1.1520,  0.3464, -0.8244,  0.5261,  1.0745,  1.3265, -1.2970]],\n",
       "\n",
       "        [[-0.8125,  0.3123,  0.1416,  0.4287, -0.5370, -1.4460,  1.9129],\n",
       "         [-1.0633,  0.5191, -0.8180,  1.2000,  1.5740, -0.6145, -0.7973],\n",
       "         [-0.9160,  0.2177,  0.5910, -1.4987, -0.2385, -0.0157,  1.8602]],\n",
       "\n",
       "        [[-0.1307, -0.7871,  0.5103, -0.6371,  1.5145,  1.0482, -1.5181],\n",
       "         [ 1.3339,  0.5637,  1.0920,  0.0866, -1.5943, -0.9185, -0.5634],\n",
       "         [ 1.0355, -0.7106, -1.0996,  1.1321, -1.3410, -0.0783,  1.0619]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slfattn(dec_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    '''\n",
    "    The complete decoder module. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    ffn_l1_out_fts = number of out_features of 1st layer in feed forward NN. Default is 2048 a suggested in the original paper\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = False, ffn_l1_out_fts = 2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        \n",
    "        # multi_head_attention sub-layer\n",
    "        self.mul_h_attn = multi_head_attn(emb_dim, h, p_drop, parallelize)\n",
    "        \n",
    "        # multi head encoder decoder attention sublayer\n",
    "        self.mul_h_enc_dec_attn = multi_head_enc_dec_attn(emb_dim, h, p_drop)\n",
    "        \n",
    "        # feedforward sublayers\n",
    "        self.l1 = nn.Linear(emb_dim, ffn_l1_out_fts)\n",
    "        self.l2 = nn.Linear(ffn_l1_out_fts, emb_dim)\n",
    "        \n",
    "        # layer norm\n",
    "        self.LNorm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, enc_vecs, dec_vecs):\n",
    "        dec_vecs = self.mul_h_attn(dec_vecs)\n",
    "        ff_in = self.mul_h_enc_dec_attn(enc_vecs, dec_vecs)\n",
    "        out = torch.relu(self.l1(ff_in))\n",
    "        out = self.drop(self.l2(out))\n",
    "        \n",
    "        return self.LNorm(out + ff_in)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder(\n",
       "  (mul_h_attn): multi_head_attn(\n",
       "    (heads): ModuleList(\n",
       "      (0): self_attention(\n",
       "        (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "      )\n",
       "      (1): self_attention(\n",
       "        (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "    (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "    (heads): ModuleList(\n",
       "      (0): encoder_decoder_attention(\n",
       "        (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "      )\n",
       "      (1): encoder_decoder_attention(\n",
       "        (WQ): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WK): Linear(in_features=7, out_features=3, bias=False)\n",
       "        (WV): Linear(in_features=7, out_features=3, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (Wo): Linear(in_features=6, out_features=7, bias=False)\n",
       "    (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (l1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "  (l2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "  (LNorm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "enc_seq_len = 4\n",
    "dec_seq_len = 2\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "enc_out = torch.randn((batch_size, enc_seq_len, emb_dim))\n",
    "dec_out = torch.randn(batch_size, dec_seq_len, emb_dim)\n",
    "dec = decoder(emb_dim, h)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 7])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ot = dec(enc_out, dec_out)\n",
    "dec_ot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(emb_dim, seq_len):\n",
    "    posts = torch.arange(seq_len).unsqueeze(1)\n",
    "    pows = 10000**(torch.arange(emb_dim//2)/float(emb_dim))\n",
    "    mat = posts/pows # rows = position in the sequence , # col = index along the embedding space\n",
    "    first_half = torch.sin(mat)\n",
    "    second_half = torch.cos(mat)\n",
    "    out = torch.cat((first_half, second_half), dim = 1)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = positional_encoding(512,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.imshow(mm.numpy(), aspect = 'auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 2, 3, 1],\n",
       "         [4, 1, 2, 4],\n",
       "         [3, 4, 4, 1]],\n",
       "\n",
       "        [[1, 0, 3, 4],\n",
       "         [0, 1, 4, 1],\n",
       "         [0, 0, 2, 3]],\n",
       "\n",
       "        [[1, 4, 0, 4],\n",
       "         [1, 2, 3, 0],\n",
       "         [2, 3, 0, 1]],\n",
       "\n",
       "        [[4, 0, 1, 2],\n",
       "         [1, 3, 3, 0],\n",
       "         [2, 4, 1, 0]],\n",
       "\n",
       "        [[3, 0, 2, 1],\n",
       "         [4, 0, 0, 3],\n",
       "         [3, 3, 3, 1]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "arr1 = torch.randint(low =0, high = 5, size = (bs, seq_len, emb_dim))\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  1.0000,  1.0000],\n",
       "        [ 0.8415,  0.0998,  0.5403,  0.9950],\n",
       "        [ 0.9093,  0.1987, -0.4161,  0.9801]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = positional_encoding(emb_dim, seq_len)\n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.0000,  2.0000,  4.0000,  2.0000],\n",
       "         [ 4.8415,  1.0998,  2.5403,  4.9950],\n",
       "         [ 3.9093,  4.1987,  3.5839,  1.9801]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  4.0000,  5.0000],\n",
       "         [ 0.8415,  1.0998,  4.5403,  1.9950],\n",
       "         [ 0.9093,  0.1987,  1.5839,  3.9801]],\n",
       "\n",
       "        [[ 1.0000,  4.0000,  1.0000,  5.0000],\n",
       "         [ 1.8415,  2.0998,  3.5403,  0.9950],\n",
       "         [ 2.9093,  3.1987, -0.4161,  1.9801]],\n",
       "\n",
       "        [[ 4.0000,  0.0000,  2.0000,  3.0000],\n",
       "         [ 1.8415,  3.0998,  3.5403,  0.9950],\n",
       "         [ 2.9093,  4.1987,  0.5839,  0.9801]],\n",
       "\n",
       "        [[ 3.0000,  0.0000,  3.0000,  2.0000],\n",
       "         [ 4.8415,  0.0998,  0.5403,  3.9950],\n",
       "         [ 3.9093,  3.1987,  2.5839,  1.9801]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1+arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_stack(nn.Module):\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = False, \n",
    "                 ffn_l1_out_fts = 2048,n_encoders = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(*[encoder(emb_dim, h, p_drop , \n",
    "                                               parallelize, \n",
    "                                               ffn_l1_out_fts)  for _ in range(n_encoders)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.stack(x)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "enc = encoder_stack(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = enc(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8923,  0.5251, -0.6445,  0.1678,  0.9223,  0.9216],\n",
       "         [ 1.5862,  0.4855,  0.2114, -0.2154, -1.7478, -0.3198],\n",
       "         [ 0.3710, -1.5323, -0.1097,  1.7951,  0.0050, -0.5290]],\n",
       "\n",
       "        [[ 0.5004,  0.4904, -0.7157, -0.9441, -1.0586,  1.7276],\n",
       "         [-1.4770, -0.2304,  0.3106,  0.7953,  1.4957, -0.8941],\n",
       "         [ 0.4796,  0.4736,  1.3118, -0.1610, -0.1617, -1.9424]],\n",
       "\n",
       "        [[-0.8948,  0.2216, -0.5275, -0.1729, -0.7094,  2.0830],\n",
       "         [-1.8062, -0.0883,  0.5582, -0.4624,  0.3578,  1.4410],\n",
       "         [ 0.8628,  0.2824, -0.0896, -1.4836, -0.9849,  1.4129]],\n",
       "\n",
       "        [[ 1.1757, -0.6365,  1.2020,  0.1115, -1.6471, -0.2056],\n",
       "         [-0.9778, -0.0948, -1.3293,  0.2655,  0.3919,  1.7446],\n",
       "         [-1.9514,  0.6208,  0.1675, -0.2584,  0.1184,  1.3030]],\n",
       "\n",
       "        [[-0.6611,  0.1051, -1.1419,  0.9845, -0.8730,  1.5864],\n",
       "         [-1.1484,  0.0979, -0.2839,  0.7939, -1.1108,  1.6513],\n",
       "         [-1.6814, -0.4246,  0.1021,  0.0486,  1.7085,  0.2469]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_stack(nn.Module):\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = False, \n",
    "                 ffn_l1_out_fts = 2048, n_decoders = 3 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.ModuleList([decoder(emb_dim, h, p_drop = 0.1, parallelize = False,\n",
    "                                             ffn_l1_out_fts = 2048) \n",
    "                                     for _ in range(n_decoders)])\n",
    "        \n",
    "    def forward(self, enc_vecs, dec_vecs):\n",
    "        for decoder in self.stack:\n",
    "            dec_vecs = decoder(enc_vecs, dec_vecs)\n",
    "        \n",
    "        # the decoder stack returns only the feature vector corresponding to \n",
    "        # the last step in decoder's input seq\n",
    "        # this is then expected to be passed to through a fully-connected network which will \n",
    "        # output the logits corresponding to the next possible word\n",
    "        return dec_vecs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "enc_seq_len = 4\n",
    "dec_seq_len = 2\n",
    "emb_dim = 7\n",
    "h = 2\n",
    "x = torch.randn((batch_size, enc_seq_len, emb_dim))\n",
    "dec_vecs = torch.randn(batch_size, dec_seq_len, emb_dim)\n",
    "enc = encoder_stack(emb_dim, h)\n",
    "dec = decoder_stack(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vecs = enc(x)\n",
    "enc_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 7])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out = dec(enc_vecs, dec_vecs)\n",
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2405, -1.1956,  0.9338, -1.3085,  1.2596,  0.8535, -0.7833],\n",
       "        [ 0.8869, -0.6120,  0.1558,  0.0080,  0.2228, -1.9960,  1.3345],\n",
       "        [ 1.2522, -0.7844,  0.8889, -0.8395, -1.2560,  1.2270, -0.4883],\n",
       "        [-0.3582,  1.5590, -0.5934, -1.4005, -0.4902, -0.0877,  1.3710],\n",
       "        [ 0.8713, -1.0543,  0.7087, -0.6222, -0.5930, -1.0062,  1.6958]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pretrained embeddings for english-french translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/prarit/.fastai/data/giga-fren/giga-fren'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/prarit/.fastai/data/giga-fren/giga-fren/giga-fren.release2.fixed.en',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/cc.fr.300.bin',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/models',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/questions_easy.csv',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/cc.en.300.bin',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/giga-fren.release2.fixed.fr',\n",
       " '/home/prarit/.fastai/data/giga-fren/giga-fren/data_save.pkl']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = glob.glob(path + '/*',)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prarit/.fastai/data/giga-fren/giga-fren/questions_easy.csv'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is light ?</td>\n",
       "      <td>Quest-ce que la lumire?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who are we?</td>\n",
       "      <td>O sommes-nous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did we come from?</td>\n",
       "      <td>D'o venons-nous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would we do without it?</td>\n",
       "      <td>Que ferions-nous sans elle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the absolute location (latitude and lo...</td>\n",
       "      <td>Quelle sont les coordonnes (latitude et longi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                                    What is light ?   \n",
       "1                                        Who are we?   \n",
       "2                            Where did we come from?   \n",
       "3                       What would we do without it?   \n",
       "4  What is the absolute location (latitude and lo...   \n",
       "\n",
       "                                                  fr  \n",
       "0                          Quest-ce que la lumire?  \n",
       "1                                    O sommes-nous?  \n",
       "2                                  D'o venons-nous?  \n",
       "3                       Que ferions-nous sans elle ?  \n",
       "4  Quelle sont les coordonnes (latitude et longi...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(p[3])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French word embeddings from fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prarit/.fastai/data/giga-fren/giga-fren/cc.fr.300.bin'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vecs = ft.load_model(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'de', '.', '</s>', 'la', 'et', ':', '', 'le', '\"']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_vecs.get_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Christine2016-09-25T00', 'OsakaVol', 'vanxains', 'Fautereau', 'IdealCoque']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_vecs.get_words()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 300)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_em = np.vstack([fr_vecs.get_word_vector(word) for word in fr_vecs.get_words()])\n",
    "fr_em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00577707,  0.04779333,  0.10935942, ..., -0.39309484,\n",
       "         0.02299723,  0.03783275],\n",
       "       [-0.08417767, -0.0387751 ,  0.04563154, ...,  0.30691683,\n",
       "        -0.01057968,  0.05789498],\n",
       "       [-0.04397016,  0.04554555,  0.02698731, ..., -0.5981208 ,\n",
       "         0.0707877 ,  0.06577325],\n",
       "       ...,\n",
       "       [ 0.006853  ,  0.02617954, -0.02090381, ..., -0.00476689,\n",
       "        -0.01509672, -0.02793315],\n",
       "       [-0.00469146,  0.01363324, -0.02618388, ..., -0.05144572,\n",
       "         0.02708546,  0.0168991 ],\n",
       "       [-0.01181357, -0.01020516,  0.00633048, ..., -0.00472526,\n",
       "         0.02741901,  0.01010639]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(fr_vecs.get_word_vector(','), fr_em[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English word embeddings from fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prarit/.fastai/data/giga-fren/giga-fren/cc.en.300.bin'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = ft.load_model(p[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'the', '.', 'and', 'to', 'of', 'a', '</s>', 'in', 'is']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vecs.get_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ldapmodify', 'sponde', 'hvm', 'GorceyBearTerritory.netSaturday', 'Zwicke']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vecs.get_words()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 300)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_em = np.vstack([en_vecs.get_word_vector(word) for word in en_vecs.get_words()])\n",
    "en_em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.25023782e-01, -1.07901648e-01,  2.45017596e-02, ...,\n",
       "         2.30474234e-01, -6.95591420e-02, -2.14496031e-02],\n",
       "       [-5.17441928e-02,  7.39639550e-02, -1.30568799e-02, ...,\n",
       "         2.37025172e-01,  4.47519124e-04, -4.19306662e-03],\n",
       "       [ 3.42323594e-02, -8.01410228e-02,  1.16187684e-01, ...,\n",
       "         5.42328537e-01, -6.24367781e-02,  9.00475308e-02],\n",
       "       ...,\n",
       "       [-6.34085387e-02, -3.75339985e-02, -2.04844430e-01, ...,\n",
       "         2.17075005e-01, -2.69116424e-02, -9.80085321e-03],\n",
       "       [ 1.42314453e-02,  2.29674261e-02, -9.92562436e-03, ...,\n",
       "         2.97835339e-02, -1.26365898e-02, -8.56679957e-03],\n",
       "       [-4.98726927e-02,  1.51695954e-02,  3.78326862e-03, ...,\n",
       "         8.17387030e-02,  3.66648473e-02,  3.68436277e-02]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(en_em[0], en_vecs.get_word_vector(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2502e-01, -1.0790e-01,  2.4502e-02,  ...,  2.3047e-01,\n",
       "         -6.9559e-02, -2.1450e-02],\n",
       "        [-5.1744e-02,  7.3964e-02, -1.3057e-02,  ...,  2.3703e-01,\n",
       "          4.4752e-04, -4.1931e-03],\n",
       "        [ 3.4232e-02, -8.0141e-02,  1.1619e-01,  ...,  5.4233e-01,\n",
       "         -6.2437e-02,  9.0048e-02],\n",
       "        ...,\n",
       "        [-6.3409e-02, -3.7534e-02, -2.0484e-01,  ...,  2.1708e-01,\n",
       "         -2.6912e-02, -9.8009e-03],\n",
       "        [ 1.4231e-02,  2.2967e-02, -9.9256e-03,  ...,  2.9784e-02,\n",
       "         -1.2637e-02, -8.5668e-03],\n",
       "        [-4.9873e-02,  1.5170e-02,  3.7833e-03,  ...,  8.1739e-02,\n",
       "          3.6665e-02,  3.6844e-02]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee = torch.tensor(en_em)\n",
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/prarit/.fastai/models')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = Config().model_path()\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc = torch.load(model_path/'fr_emb.pth')\n",
    "emb_dec = torch.load(model_path/'en_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8144, 300, padding_idx=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2496,  2.0719, -0.8209,  ...,  0.0863,  1.6173, -0.4306],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3272,  1.0044, -1.7942,  ...,  0.4120, -0.3864, -0.1713],\n",
       "        ...,\n",
       "        [ 0.7001, -1.7252,  1.5785,  ..., -1.7353,  0.5184, -1.5111],\n",
       "        [ 0.0386, -0.3759, -0.4225,  ...,  1.5592, -0.3207,  0.4177],\n",
       "        [-0.8857,  0.0890,  1.4376,  ...,  0.3963,  1.4328, -0.6964]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dec.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(11336, 300, padding_idx=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_emb, dec_emb, num_heads, \n",
    "                 n_encoders = 3, n_decoders = 3,\n",
    "                 p_drop = 0.1, ffn_l1_out_fts = 2048, \n",
    "                 pad_idx = 1, bos_idx = 0, max_sq_len = 30,\n",
    "                 positional_encoding_func = positional_encoding, \n",
    "                 parallelize = False ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.max_sq_len = max_sq_len\n",
    "        \n",
    "        # encoder \n",
    "        self.enc_emb = nn.Embedding.from_pretrained(enc_emb, freeze = False,\n",
    "                                                     padding_idx = pad_idx)\n",
    "        \n",
    "        self.emb_dim = self.enc_emb.embedding_dim # dimension of embedding vectors\n",
    "        \n",
    "        self.h = num_heads\n",
    "        self.p_drop = p_drop\n",
    "        \n",
    "        self.positional_encoding = positional_encoding_func\n",
    "        \n",
    "        self.drop_input = nn.Dropout(p_drop)\n",
    "        \n",
    "        self.encoder = encoder_stack(self.emb_dim, num_heads, p_drop = p_drop, \n",
    "                                     parallelize = parallelize,\n",
    "                                     ffn_l1_out_fts = ffn_l1_out_fts,\n",
    "                                     n_encoders = n_encoders)\n",
    "        \n",
    "        # decoder \n",
    "        \n",
    "        self.dec_emb = nn.Embedding.from_pretrained(dec_emb, freeze = False, \n",
    "                                                    padding_idx = pad_idx)\n",
    "        self.num_words = self.dec_emb.num_embeddings \n",
    "        self.decoder = decoder_stack(self.emb_dim, num_heads, p_drop = p_drop, \n",
    "                                     parallelize = parallelize,\n",
    "                                     ffn_l1_out_fts = ffn_l1_out_fts, \n",
    "                                     n_decoders = n_decoders)\n",
    "        \n",
    "        # logits from decoder output\n",
    "        self.logits = nn.Linear(self.emb_dim, self.num_words)\n",
    "        # we will tie the weights of the logits layer to that of the dec_embeddings\n",
    "        # that this improves translation was suggested in the following paper\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # this was also done in the transformer paper\n",
    "        self.logits.weight = self.dec_emb.weight\n",
    "        # The above weight tying is identical to the one done in the following pytorch example\n",
    "        # https://github.com/pytorch/examples/blob/master/word_language_model/model.py#L28\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # has shape (batch_size, seq_length); entries in a sequence correspond to word indices\n",
    "        batch_size, enc_seq_len = x.shape\n",
    "        enc_embeddings = self.enc_emb(x) # embeddings to input to the encoder\n",
    "        # add positional encodings to the encoder's input embeddings\n",
    "        enc_pe = self.positional_encoding(self.emb_dim, enc_seq_len)\n",
    "        enc_in = self.drop_input(enc_embeddings + enc_pe)\n",
    "        enc_out = self.encoder(enc_in)\n",
    "        dec_in_seq = torch.tensor([[self.bos_idx]]*batch_size).to(device = x.device)\n",
    "        # dec_in_seq contains word indices obtained  from the decoder; Initialised to bos_idx\n",
    "        out_seq_logits = []\n",
    "        for itr in range(self.max_sq_len):\n",
    "            dec_embeddings = self.dec_emb(dec_in_seq) \n",
    "            dec_seq_len = dec_in_seq.shape[1]\n",
    "            # add positional encodings to dec_embeddings\n",
    "            dec_pe = self.positional_encoding(self.emb_dim, dec_seq_len)\n",
    "            dec_in = self.drop_input(dec_embeddings + dec_pe)\n",
    "            dec_out = self.decoder(enc_out, dec_in)\n",
    "            # recall that the decoder_stack always returns the feature vector corresponding\n",
    "            # to the last word in the decoder input sequence. Therefore, dec_out will be of\n",
    "            # shape (batch_size, emb_dim)\n",
    "            # we need to pass this through a dense layer to convert it to \n",
    "            # logits for the output words; probs are obtained by applying softmax activation\n",
    "            # recall that pytorch cross entropy loss combines log_softmax and NLLLoss\n",
    "            # so here we will not apply softmax and pass logits to the loss function\n",
    "            logits = self.logits(dec_out)\n",
    "            #probs = F.softmax(self.logits(dec_out), dim = 1)\n",
    "            # logits as well as probs, have shape = batch_size x num_words\n",
    "            out_seq_logits.append(logits)\n",
    "            #next_word = probs.max(dim = 1)[1]\n",
    "            next_word = logits.argmax(dim = 1)\n",
    "            if all(next_word == self.pad_idx): break\n",
    "            dec_in_seq = torch.cat((dec_in_seq, next_word.unsqueeze(1)), axis = 1)\n",
    "        \n",
    "        return torch.stack(out_seq_logits, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]),\n",
       " Transformer(\n",
       "   (enc_emb): Embedding(11336, 300, padding_idx=1)\n",
       "   (drop_input): Dropout(p=0.1, inplace=False)\n",
       "   (encoder): encoder_stack(\n",
       "     (stack): Sequential(\n",
       "       (0): encoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): encoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): encoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (dec_emb): Embedding(8144, 300, padding_idx=1)\n",
       "   (decoder): decoder_stack(\n",
       "     (stack): ModuleList(\n",
       "       (0): decoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): decoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): decoder(\n",
       "         (mul_h_attn): multi_head_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): self_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "           (heads): ModuleList(\n",
       "             (0): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "             (1): encoder_decoder_attention(\n",
       "               (WQ): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WK): Linear(in_features=300, out_features=150, bias=False)\n",
       "               (WV): Linear(in_features=300, out_features=150, bias=False)\n",
       "             )\n",
       "           )\n",
       "           (Wo): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "         (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "         (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "         (drop): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (logits): Linear(in_features=300, out_features=8144, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "num_heads = 2\n",
    "max_sq_len = 4\n",
    "x = torch.randint(low = 0, high =11335, size = (batch_size, seq_len) )\n",
    "transf = Transformer(emb_enc.weight.data, emb_dec.weight.data, num_heads, \n",
    "                     max_sq_len = max_sq_len)\n",
    "x.shape, transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8421, 10808,  4863],\n",
       "        [ 5148, 11225,  5065],\n",
       "        [ 6909,  3452, 10796],\n",
       "        [ 1863,  6190, 10033],\n",
       "        [ 8106,  9612,  3648]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 8144])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = transf(x)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7312e+02, -2.2276e-02, -1.3939e+01,  ..., -1.1534e+01,\n",
       "           5.2347e+00,  3.0403e+01],\n",
       "         [ 1.7449e+02, -2.2276e-02, -1.2019e+01,  ...,  6.9383e+00,\n",
       "          -6.8235e+00,  2.7640e+01],\n",
       "         [ 1.8691e+02, -2.2276e-02, -2.6299e+01,  ..., -3.1926e+00,\n",
       "          -2.5973e+00,  4.7556e+01],\n",
       "         [ 1.8889e+02, -2.2276e-02, -7.2559e+00,  ...,  6.9504e+00,\n",
       "          -4.1486e+00,  4.6276e+01]],\n",
       "\n",
       "        [[ 1.7657e+02, -2.2276e-02, -1.1314e+01,  ..., -7.0387e+00,\n",
       "          -9.8641e-01,  4.6491e+01],\n",
       "         [ 1.8943e+02, -2.2276e-02, -2.4778e+01,  ..., -1.7506e+01,\n",
       "           1.4152e+01,  2.2249e+01],\n",
       "         [ 1.8305e+02, -2.2276e-02, -1.4829e+01,  ..., -1.0529e+01,\n",
       "           2.1773e+00,  3.2768e+01],\n",
       "         [ 1.7934e+02, -2.2276e-02, -3.5629e+00,  ..., -1.1500e+01,\n",
       "           4.8173e+00,  4.6427e+01]],\n",
       "\n",
       "        [[ 1.6747e+02, -2.2276e-02, -7.6507e+00,  ..., -1.3752e+00,\n",
       "          -4.7548e+00,  4.5691e+01],\n",
       "         [ 1.7722e+02, -2.2276e-02, -1.3602e+01,  ...,  3.3635e+00,\n",
       "          -6.3059e-01,  2.7625e+01],\n",
       "         [ 1.7874e+02, -2.2276e-02, -1.8551e+01,  ..., -5.4808e+00,\n",
       "           2.1091e+00,  4.0797e+01],\n",
       "         [ 1.8581e+02, -2.2276e-02, -1.0005e+01,  ...,  8.9055e-01,\n",
       "           9.9523e+00,  4.5461e+01]],\n",
       "\n",
       "        [[ 1.6199e+02, -2.2276e-02, -1.3401e+01,  ..., -6.1605e+00,\n",
       "           2.0930e+00,  4.9171e+01],\n",
       "         [ 1.7685e+02, -2.2276e-02, -1.8363e+01,  ..., -6.6400e+00,\n",
       "          -5.9851e-01,  4.3949e+01],\n",
       "         [ 1.7792e+02, -2.2276e-02, -8.7654e+00,  ..., -8.7632e+00,\n",
       "           1.2775e+01,  4.3102e+01],\n",
       "         [ 1.6633e+02, -2.2276e-02, -5.6434e-01,  ..., -1.0937e+01,\n",
       "          -3.4577e+00,  3.7239e+01]],\n",
       "\n",
       "        [[ 1.8087e+02, -2.2276e-02, -4.5146e+00,  ..., -8.4681e+00,\n",
       "           8.3992e+00,  4.7405e+01],\n",
       "         [ 1.9339e+02, -2.2276e-02, -1.6396e+01,  ..., -1.2968e+01,\n",
       "          -5.4452e+00,  4.2161e+01],\n",
       "         [ 1.7425e+02, -2.2276e-02, -2.1554e+00,  ..., -1.2303e+00,\n",
       "           2.7743e+00,  2.8131e+01],\n",
       "         [ 1.7617e+02, -2.2276e-02,  1.1790e+01,  ..., -6.2338e+00,\n",
       "          -3.5733e+00,  3.3130e+01]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "class self_attention(nn.Module):\n",
    "    '''\n",
    "    Module to apply self attention to an input sequence of vectors\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vector\n",
    "    h = number of self attention heads\n",
    "    mask = whether to prevent positions from attending to subsequent positions  \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, mask = False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim//h\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Querry vector\n",
    "        self.WQ = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WK = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        self.WV = nn.Linear(emb_dim, self.red_vec_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_len, emb_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        if self.mask:\n",
    "            mask = torch.triu(torch.ones((seq_len, seq_len))*(-float('inf')), \n",
    "                          diagonal = 1).to(device = x.device)\n",
    "        else:\n",
    "            mask = torch.zeros((seq_len, seq_len)).to(device = x.device)\n",
    "        querries = self.WQ(x)\n",
    "        keys = self.WK(x)\n",
    "        values = self.WV(x)\n",
    "        pre_sftmx = querries@keys.permute(0,2,1)/np.sqrt(self.red_vec_size) + mask\n",
    "        att_scores = F.softmax(pre_sftmx, dim = 2)\n",
    "        ctx_vecs = att_scores @ values \n",
    "        assert ctx_vecs.shape == (batch_size, seq_len, self.red_vec_size ) \n",
    "        return querries, keys, values, att_scores, ctx_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "mask = torch.triu(torch.ones((seq_len, seq_len))*(-float('inf')), diagonal = 1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(mask, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 1\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = self_attention(emb_dim, h, mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention(\n",
       "  (WQ): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WK): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (WV): Linear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "q , k, v, s, c = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.4468, 0.5532, 0.0000],\n",
       "         [0.3600, 0.3397, 0.3003]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.5390, 0.4610, 0.0000],\n",
       "         [0.3008, 0.1250, 0.5741]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.5111, 0.4889, 0.0000],\n",
       "         [0.2980, 0.2189, 0.4831]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.4778, 0.5222, 0.0000],\n",
       "         [0.3447, 0.3546, 0.3007]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.0642, 0.9358, 0.0000],\n",
       "         [0.2382, 0.4721, 0.2897]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn2 = self_attention(emb_dim, h, mask = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "q , k, v, s, c = attn2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing speed of map() vs for loops to iterate over heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, going over each head iteratively is only slightly faster than using map. For a batch_size = 32, seq_len = 30, emb_dim = 300 and heads = 6, I got the following averge timings (averaged over a 1000 trials):\n",
    "\n",
    " 1) using map(): time ~ 0.007012611627578736\n",
    " \n",
    " 2) using loops: time ~ 0.0064054858684539796\n",
    "\n",
    "Also see [this](https://discuss.pytorch.org/t/mapping-function-to-tensor/66089/2) discussion about using the map function in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn_new(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = nn.ModuleList([self_attention(emb_dim, h) for i in range(h)])\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        start = time.time()\n",
    "        ctx_vecs = torch.cat(tuple(map(lambda head: head(x)[4], self.heads)), dim = 2)\n",
    "        end = time.time()\n",
    "        time_map = end - start\n",
    "        \n",
    "        start = time.time()\n",
    "        ctx_vecs2 = torch.cat([head(x)[4] for head in self.heads], dim = 2)\n",
    "        end = time.time()\n",
    "        time_iter = end - start\n",
    "        \n",
    "        transformed = self.drop(self.Wo(ctx_vecs))\n",
    "        return time_map, time_iter, ctx_vecs, ctx_vecs2, self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = multi_head_attn_new(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap, titer, ctx1, ctx2, out = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004120826721191406"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007688999176025391"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 30\n",
    "emb_dim = 300\n",
    "h = 6\n",
    "attn = multi_head_attn_new(emb_dim, h)\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap, titer, ctx1, ctx2, out = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_map = []\n",
    "time_iter = []\n",
    "for _ in range(1000):\n",
    "    x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "    tmap, titer, ctx1, ctx2, out = attn(x)\n",
    "    if not (ctx1 == ctx2).all():\n",
    "        print('the two vectors dont match')\n",
    "        break\n",
    "    time_map.append(tmap)\n",
    "    time_iter.append(titer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007012611627578736"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0064054858684539796"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trials in using multiprocessing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to the multiprocessing module to parallelize the execution of the different heads but couldn't get it to work. This is what I tried: I figured that i can use the multiprocessing's Pool object to parallely scan over the list of heads and apply them to the input sequence. Towards this end I had to create a function ```apply_head``` that which would return the result of apply the head passed to it i.e. \n",
    "\n",
    "``` def apply_head(head): return head(x)```\n",
    "\n",
    "The idea was that I can then pass ```apply_head``` as the target function to pool i.e.\n",
    "\n",
    "``` ctx_vecs_pooled = p.map(apply_head, self.heads) ```,\n",
    "\n",
    "similar to the way pool was used in the example on [this](https://docs.python.org/3/library/multiprocessing.html) official multiprocessing documentation.\n",
    "\n",
    "Unfortunately this fails and throws a ```Can't pickle local object``` error. This has also been described in the following [stackoverflow post](https://stackoverflow.com/questions/8804830/python-multiprocessing-picklingerror-cant-pickle-type-function). Unfortunately, I couldn't figure out a way around it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attn_new(nn.Module):\n",
    "    '''\n",
    "    Module to create multiple attention heads\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    emb_dim = dimension of the embedding vectors\n",
    "    h = number of attention heads\n",
    "    parallelize = parallelize the computations for differnt heads \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, emb_dim, h, p_drop = 0.1, parallelize = 'False'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h = h\n",
    "        self.red_vec_size = emb_dim // h \n",
    "        \n",
    "        self.heads = nn.ModuleList([self_attention(emb_dim, h) for i in range(h)])\n",
    "        \n",
    "        # transform the contatenated context vectors to have same size as emb_sim\n",
    "        # this is to be able to enable implement a skip-connection between the input and output\n",
    "        self.Wo = nn.Linear(self.red_vec_size*h, emb_dim, bias = False) \n",
    "        \n",
    "        # layer norm\n",
    "        # should we apply \n",
    "        self.LNorm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        num_processes = 4\n",
    "        start = time.time()\n",
    "        \n",
    "        def apply_head(head):\n",
    "            return head(x)\n",
    "        \n",
    "        processes = []\n",
    "        \n",
    "        with Pool(num_processes) as p:\n",
    "            ctx_vecs_pooled = p.map(apply_head, self.heads)\n",
    "        \n",
    "        ctx_vecs = torch.cat(ctx_vecs_pooled, dim = 2)\n",
    "        end = time.time()\n",
    "        time_map = end - start\n",
    "        \n",
    "        start = time.time()\n",
    "        ctx_vecs2 = torch.cat([head(x)[4] for head in self.heads], dim = 2)\n",
    "        end = time.time()\n",
    "        time_iter = end - start\n",
    "        \n",
    "        transformed = self.drop(self.Wo(ctx_vecs))\n",
    "        return time_map, time_iter, ctx_vecs_pooled, ctx_vecs, ctx_vecs2, self.LNorm(x + transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 3\n",
    "emb_dim = 4\n",
    "h = 2\n",
    "x = torch.randn((batch_size, seq_len, emb_dim))\n",
    "attn = multi_head_attn_new(emb_dim, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'multi_head_attn_new.forward.<locals>.apply_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-3029c2f1e55f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-4555035ab9d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mctx_vecs_pooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mctx_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_vecs_pooled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    422\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'multi_head_attn_new.forward.<locals>.apply_head'"
     ]
    }
   ],
   "source": [
    "tmap, titer, ctx1, ctx2, out = attn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/8804830/python-multiprocessing-picklingerror-cant-pickle-type-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_head(head): return head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_list = [head_factory(i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'head_factory.<locals>.head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-cef50569972f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mctx_vecs_pooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    422\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'head_factory.<locals>.head'"
     ]
    }
   ],
   "source": [
    "num_processes = 4\n",
    "        \n",
    "with Pool(num_processes) as p:\n",
    "    ctx_vecs_pooled = p.map(apply_head, head_list)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
