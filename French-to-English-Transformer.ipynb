{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastai.text import *\n",
    "from Transformer import Transformer as my_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for preprocessing data and Bleu-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n",
    "    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n",
    "    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
    "        else:         \n",
    "            res_x[i,:len(s[0])],res_y[i,:len(s[1])] = LongTensor(s[0]),LongTensor(s[1])\n",
    "    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n",
    "    return res_x,res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n",
    "               dl_tfms=None, pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Config().data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Config().data_path()/'giga-fren'/'giga-fren'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos quel est le risque de contracter la maladie x après avoir été exposé à des facteurs de risque environnementaux , comme des polluants dans la zone de résidence ?</td>\n",
       "      <td>xxbos what is the risk of developing disease x after exposure to environmental risk factors , such as pollutants in the area of residence ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos quelle importance votre organisation xxunk - t - elle au traitement non discriminatoire et égal des immigrés comme condition préalable au développement de la diversité culturelle en europe ?</td>\n",
       "      <td>xxbos which importance does your organisation attach to a non - discriminatory , equal treatment of immigrants as a xxunk for the development of cultural diversity in european societies ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos quelles sont vos opinions sur le sujet et quelle est la meilleure façon de faire xxunk un concept pour arriver à une technologie viable sur le plan commercial ?</td>\n",
       "      <td>xxbos what are some of your thoughts on this subject and best way of moving from an idea to a commercially viable technology ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos qu'en est - il dans le cas de la xxunk in vitro où des centaines de milliers d'entre eux sont xxunk sans être xxunk à aucun projet parental ?</td>\n",
       "      <td>xxbos what about cases of xxunk treatment where hundreds of thousands of these embryos are frozen without being assigned to a subsequent pregnancy ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos quelles conditions particulières doivent être remplies pendant l’examen préliminaire international en ce qui concerne les xxunk des séquences de nucléotides ou d’acides aminés ou les tableaux y relatifs ?</td>\n",
       "      <td>xxbos what special requirements apply during the international preliminary examination to nucleotide and / or amino acid sequence listings and / or tables related thereto ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting\n",
    "\n",
    "Add a transformation function that add shifted targets to the inputs of the model. This is a copy of an identical preprocessing done in fastai's transformer notebook provided with the 2019 NLP course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = data.vocab\n",
    "v.stoi['xxpad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tfm(b):\n",
    "    x,y = b\n",
    "    y = F.pad(y, (1, 0), value=1)\n",
    "    return [x,y[:,:-1]], y[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell twice\n",
    "data.add_tfm(shift_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos qu’est - ce que la lumière ?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = iter(data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(traindl)), len(next(traindl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    2,    27,   580,  5212,    11,    74,    14,    45,    10,  1337,\n",
       "            188,     9,     1],\n",
       "         [    2,    24,    20,    12,   104,    25,    72,  2314,    11,  3821,\n",
       "           1617,     9,     1],\n",
       "         [    2,    33,    22,    11,    28,    19,    63,   183,    22,    51,\n",
       "            562,     9,     1],\n",
       "         [    2,    19,   134,    11,    29,    52,    21,  4197,    12,  6547,\n",
       "           3763,     9,     1],\n",
       "         [    2,    65,    11,    28,    23,  1147,    39,   111,   111,     0,\n",
       "           1236,     9,     1],\n",
       "         [    2,    27,   433,    10,    87,   223,  3352,    66,    11,    43,\n",
       "           5233,     9,     1],\n",
       "         [    2,    27,    20,    12,  3532,    12,    45,     0,    10,    46,\n",
       "           1484,     9,     1],\n",
       "         [    2,    19,    59,  2930,    11,    29,    51,    28,    72,  7861,\n",
       "             48,     9,     1],\n",
       "         [    2,    23,    40,    13,   360,    10,   600,    12,   104,    25,\n",
       "            779,     9,     1],\n",
       "         [    2,    23,    66,   380,    41,   327,    44,  6026,    10,    13,\n",
       "            231,     9,     1],\n",
       "         [    2,    24,    20,    12,   266,    21,  3806,    25,   378,    10,\n",
       "           1240,     9,     1],\n",
       "         [    2,    33,  1706,    11,    34,    11,    43,    51,   949,    13,\n",
       "           2720,     9,     1],\n",
       "         [    2,    35,    22,    13,   422,    16,   410,   203,    42,    13,\n",
       "            231,     9,     1],\n",
       "         [    2,    33,     0,    11,    56,    26,   285,    45,    36,    13,\n",
       "          10832,     9,     1],\n",
       "         [    2,    24,  7331,    12,   123,  1091,  1680,  6514,    10,    13,\n",
       "             87,     9,     1],\n",
       "         [    2,     0,    35,   477,  7738,    37,    10,  4697,   134,    11,\n",
       "             29,     9,     1],\n",
       "         [    2,    32,   106,    14,   534,    25,   799,  3294,    58,     0,\n",
       "             47,     9,     1],\n",
       "         [    2,    35,   681,    37,    32,  1116,    40,    13,    45,  1369,\n",
       "            260,     9,     1],\n",
       "         [    2,    27,   580,  2206,  2749,    12,   166,   749,    10,   100,\n",
       "            122,     9,     1],\n",
       "         [    2,    23,    22,    14,   606,   314,  2070,   359,  2344,    16,\n",
       "            348,     9,     1],\n",
       "         [    2,    27,    60,    64,    12,   325,    12,    45,   792,    15,\n",
       "             33,     9,     1],\n",
       "         [    2,    70,     0,    11,    57,    19,   150,  2444,    40,    64,\n",
       "           4831,     9,     1],\n",
       "         [    2,    15,    12,   586,    30,    14,   132,    10,   218,    10,\n",
       "             68,     9,     1],\n",
       "         [    2,    32,   141,    10,   311,    10,   929,   551,    11,    29,\n",
       "            638,     9,     1],\n",
       "         [    2,    27,    20,    12,   253,   144,    15,   659,    17,   170,\n",
       "            882,     9,     1],\n",
       "         [    2,    33,   655,    11,    56,    17,  1002,    39,   244,   128,\n",
       "           1123,     9,     1],\n",
       "         [    2,    27,   495,   281,    99,    62,  1141,  2704,    38,  1966,\n",
       "           2106,     9,     1],\n",
       "         [    2,    24,    80,   129,    99,  3473,    36,    14,   450,    10,\n",
       "           4390,     9,     1],\n",
       "         [    2,    23,    22,  3809,    17,    13,   121,    36,    12,   424,\n",
       "           1234,     9,     1],\n",
       "         [    2,    55,    11,    28,    23,    31,  4683,    17,    52,    16,\n",
       "           2963,     9,     1],\n",
       "         [    2,    24,    20,    12,   196,    10,   388,   726,  7246,    14,\n",
       "            534,     9,     1],\n",
       "         [    2,    67,   146,    11,    49,   232,    14,  2256,    78,    12,\n",
       "            938,     9,     1],\n",
       "         [    2,    17,    27,   144,   842,     0,    11,    34,    11,    49,\n",
       "             17,   498,     9],\n",
       "         [    2,    24,   120,    12,   240,    26,    96,    10,   145,    15,\n",
       "             10,   604,     9],\n",
       "         [    2,    19,  3539,    41,   205,    23,    69,     0,    48,  4913,\n",
       "             16,  4441,     9],\n",
       "         [    2,    55,    11,    28,   211,   722,   741,  1677,    73,  2172,\n",
       "           6377,    77,     9],\n",
       "         [    2,    32,    22,    14,    83,    25,    90,   368,    30,    14,\n",
       "            390,   553,     9],\n",
       "         [    2,   326,    58,   451,    47,    26,  2244,    58,  1831,    47,\n",
       "             14,   345,     9],\n",
       "         [    2,    19,  3025,    11,    31,    62,    39,   676,  2061,    75,\n",
       "             12,   209,     9],\n",
       "         [    2,    70,    89,    11,    43,    59,  9713,   107,  1442,    37,\n",
       "             97,  5799,     9],\n",
       "         [    2,    27,    20,    12,   225,   435,    10,    28,  2310,  6955,\n",
       "             16,  1446,     9],\n",
       "         [    2,    35,    22,    13,   175,   443,    10,  4703,    26,    82,\n",
       "             10,   607,     9],\n",
       "         [    2,    24,   375,  2382,    11,    31,    21, 10344,    14,  2919,\n",
       "             10,     0,     9],\n",
       "         [    2,    23,    66,   395,    14,   463,    21,    41,   169,    17,\n",
       "            732,   493,     9],\n",
       "         [    2,    17,    71,   337,    11,    31,     0,   112,    31,   337,\n",
       "             17,  3134,     9],\n",
       "         [    2,    26,    71,  1190,    12,   290,    10,   331,    21,    13,\n",
       "             68,   233,     9],\n",
       "         [    2,    33,   452,   536,    20,    11,    74,    26,    68,    15,\n",
       "            249,   131,     9],\n",
       "         [    2,    33,    88,    11,    31,  1383,    10,  4498,  1122,    36,\n",
       "             13,   769,     9],\n",
       "         [    2,    70,  5627,    11,    31,    48,   665,    10,   588,    39,\n",
       "           3957,  9568,     9],\n",
       "         [    2,    35,    22,  1636,  2992,     0,    58,     0,    47,    10,\n",
       "             46,   897,     9],\n",
       "         [    2,    10,    27,  2756,    20,    12,  7400,    19,    56,     0,\n",
       "             15,    33,     9],\n",
       "         [    2,    19,     0,    11,    31,    44,   166,  2291,    38,   148,\n",
       "             25,   109,     9],\n",
       "         [    2,    61,    98,    11,    57,   187,    14,   416,  5462,    21,\n",
       "             14,     0,     9],\n",
       "         [    2,    32,    22,    14,   343,     0,    38,   318,    10,    13,\n",
       "            260,   309,     9],\n",
       "         [    2,    55,    11,    28,    19,    13,     0,  9945,     0,    58,\n",
       "              0,    47,     9],\n",
       "         [    2,    61,    15,    17,    27,   235,    40,    11,    34,    11,\n",
       "             29,   191,     9],\n",
       "         [    2,    35,   477,    10,   538,    14,   132,  1026,    11,    29,\n",
       "             10,   588,     9],\n",
       "         [    2,    27,   615,     0,    11,    31,    21,    69,    48,  5110,\n",
       "             10,  3067,     9],\n",
       "         [    2,    32,    85,    62,    14,    83,    25,   176,  2754,    30,\n",
       "             28,   202,     9],\n",
       "         [    2,    33,    63,  1150,    22,    11,    49,    13,    45,   562,\n",
       "             10,  1376,     9],\n",
       "         [    2,    24,    20,    12,   985,    17,   214,    21,   499,  2561,\n",
       "             25,  3151,     9],\n",
       "         [    2,    67,     0,  6341,   310,    23,  5216,    12,  1435,   107,\n",
       "            360,     0,     9],\n",
       "         [    2,    33,    14,    72,    16,  3720,    22,    11,    29,   859,\n",
       "           3306,   924,     9],\n",
       "         [    2,    33,    40,    11,    34,    11,    43,   139,   110,   128,\n",
       "             10,   858,     9]], device='cuda:0'),\n",
       " tensor([[ 1,  2, 39,  ...,  1,  1,  1],\n",
       "         [ 1,  2, 11,  ...,  1,  1,  1],\n",
       "         [ 1,  2, 22,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  2, 11,  ...,  1,  1,  1],\n",
       "         [ 1,  2, 22,  ...,  1,  1,  1],\n",
       "         [ 1,  2, 22,  ...,  1,  1,  1]], device='cuda:0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_batch = next(traindl)\n",
    "inp_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Config().model_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc = torch.load(model_path/'fr_emb.pth')\n",
    "emb_dec = torch.load(model_path/'en_emb.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and accuracy functions\n",
    "\n",
    "This is copy of fastai's implementation of the bleu metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(out, targ, pad_idx=1):\n",
    "    bs,targ_len = targ.size()\n",
    "    _,out_len,vs = out.size()\n",
    "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "    return CrossEntropyFlat()(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_acc(out, targ, pad_idx=1):\n",
    "    bs,targ_len = targ.size()\n",
    "    _,out_len,vs = out.size()\n",
    "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "    out = out.argmax(2)\n",
    "    return (out==targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram():\n",
    "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "    def __eq__(self, other):\n",
    "        if len(self.ngram) != len(other.ngram): return False\n",
    "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grams(x, n, max_n=5000):\n",
    "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
    "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
    "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(learn, ds_type=DatasetType.Valid):\n",
    "    learn.model.eval()\n",
    "    inputs, targets, outputs = [],[],[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in progress_bar(learn.dl(ds_type)):\n",
    "            out = learn.model(*xb).cpu()\n",
    "            for x,y,z in zip(xb[0],yb,out):\n",
    "                inputs.append(learn.data.train_ds.x.reconstruct(x.cpu()))\n",
    "                targets.append(learn.data.train_ds.y.reconstruct(y.cpu()))\n",
    "                outputs.append(learn.data.train_ds.y.reconstruct(z.cpu().argmax(1)))\n",
    "    return inputs, targets, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusBLEU(Callback):\n",
    "    def __init__(self, vocab_sz):\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.name = 'bleu'\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        last_output = last_output.argmax(dim=-1)\n",
    "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "            self.pred_len += len(pred)\n",
    "            self.targ_len += len(targ)\n",
    "            for i in range(4):\n",
    "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                self.corrects[i] += c\n",
    "                self.counts[i]   += t\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n",
    "        return add_metrics(last_metrics, bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (enc_emb): Embedding(11336, 300, padding_idx=1)\n",
       "  (drop_input): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): encoder_stack(\n",
       "    (stack): Sequential(\n",
       "      (0): encoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): encoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): encoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_emb): Embedding(8144, 300, padding_idx=1)\n",
       "  (decoder): decoder_stack(\n",
       "    (stack): ModuleList(\n",
       "      (0): decoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): decoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): decoder(\n",
       "        (mul_h_attn): multi_head_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): self_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mul_h_enc_dec_attn): multi_head_enc_dec_attn(\n",
       "          (heads): ModuleList(\n",
       "            (0): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (1): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (2): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (3): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (4): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (5): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (6): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "            (7): encoder_decoder_attention(\n",
       "              (WQ): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WK): Linear(in_features=300, out_features=37, bias=False)\n",
       "              (WV): Linear(in_features=300, out_features=37, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=296, out_features=300, bias=False)\n",
       "          (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (l1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (l2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (LNorm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (logits): Linear(in_features=300, out_features=8144, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "transformer = my_transformer(emb_enc.weight.data, emb_dec.weight.data, num_heads = num_heads)\n",
    "transformer = transformer.cuda()\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(transformer.parameters())).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a learner based on the transformer\n",
    "learn = Learner(data, transformer, loss_func=seq2seq_loss, \n",
    "                metrics=[seq2seq_acc, CorpusBLEU(len(data.train_ds.y.vocab.itos))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>seq2seq_acc</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='92' class='' max='604', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.23% [92/604 00:10<00:59 66.2961]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>seq2seq_acc</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.862966</td>\n",
       "      <td>3.173168</td>\n",
       "      <td>0.563059</td>\n",
       "      <td>0.421688</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.452380</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>0.608266</td>\n",
       "      <td>0.450832</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.093873</td>\n",
       "      <td>2.323136</td>\n",
       "      <td>0.640868</td>\n",
       "      <td>0.465310</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.837251</td>\n",
       "      <td>2.007134</td>\n",
       "      <td>0.666379</td>\n",
       "      <td>0.480771</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.689504</td>\n",
       "      <td>1.770551</td>\n",
       "      <td>0.695614</td>\n",
       "      <td>0.511168</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.510879</td>\n",
       "      <td>1.622698</td>\n",
       "      <td>0.714669</td>\n",
       "      <td>0.527113</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.340519</td>\n",
       "      <td>1.554478</td>\n",
       "      <td>0.724518</td>\n",
       "      <td>0.539455</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.275086</td>\n",
       "      <td>1.542626</td>\n",
       "      <td>0.726145</td>\n",
       "      <td>0.540960</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, 5e-4, div_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>seq2seq_acc</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.262238</td>\n",
       "      <td>1.541084</td>\n",
       "      <td>0.725805</td>\n",
       "      <td>0.540532</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323489</td>\n",
       "      <td>1.536113</td>\n",
       "      <td>0.727106</td>\n",
       "      <td>0.543081</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.239091</td>\n",
       "      <td>1.467378</td>\n",
       "      <td>0.734355</td>\n",
       "      <td>0.550555</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.081250</td>\n",
       "      <td>1.387838</td>\n",
       "      <td>0.747349</td>\n",
       "      <td>0.565355</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.918348</td>\n",
       "      <td>1.337473</td>\n",
       "      <td>0.757211</td>\n",
       "      <td>0.578829</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.775400</td>\n",
       "      <td>1.288458</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.589193</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.704190</td>\n",
       "      <td>1.285699</td>\n",
       "      <td>0.769347</td>\n",
       "      <td>0.595464</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.633310</td>\n",
       "      <td>1.286594</td>\n",
       "      <td>0.770419</td>\n",
       "      <td>0.597021</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, 5e-4, div_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='151' class='' max='151', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [151/151 00:10<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets, outputs = get_predictions(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos quelles méthodes a - t - on trouvées particulièrement efficaces pour consulter le public et les parties intéressées sur la protection des renseignements personnels reliés à la santé ?',\n",
       " 'xxbos what approaches have been found particularly effective in consulting with the public and stakeholders on the protection of personal health information ?',\n",
       " 'xxbos what methods have been found particularly effective in addressing the the public and stakeholders on the protection of personal health information ?')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[10].text,targets[10].text,outputs[10].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"xxbos qui a le pouvoir de modifier le règlement sur les poids et mesures et le règlement sur l'inspection de l'électricité et du gaz ?\",\n",
       " 'xxbos who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations ?',\n",
       " 'xxbos who has the authority to change the regulations and control measures measures ? to regulations and regulations of ?')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[700].text,targets[700].text,outputs[700].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos ´ ` ou sont xxunk leurs grandes convictions en ce qui a trait a la ` ` ´ transparence et a la responsabilite ?',\n",
       " 'xxbos what happened to their great xxunk about transparency and accountability ?',\n",
       " 'xxbos what are to them views beliefs beliefs transparency and accountability ?')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[701].text,targets[701].text,outputs[701].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos de quoi l’afrique a - t - elle vraiment besoin pour se sortir de la pauvreté ?',\n",
       " 'xxbos what does africa really need to pull itself out of poverty ?',\n",
       " 'xxbos what does africa really need to pull out out of poverty ?')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2500].text,targets[2500].text,outputs[2500].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos quelles ressources votre communauté possède - t - elle qui favoriseraient la guérison ?',\n",
       " 'xxbos what resources exist in your community that would promote recovery ?',\n",
       " 'xxbos what resources does in your community that would help healing ?')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[4002].text,targets[4002].text,outputs[4002].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions without inputing ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions2(learn, ds_type=DatasetType.Valid):\n",
    "    learn.model.eval()\n",
    "    inputs, targets, outputs = [],[],[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in progress_bar(learn.dl(ds_type)):\n",
    "            out = learn.model.predict(xb[0].cuda()).cpu()\n",
    "            for x,y,z in zip(xb[0],yb,out):\n",
    "                inputs.append(learn.data.train_ds.x.reconstruct(x.cpu()))\n",
    "                targets.append(learn.data.train_ds.y.reconstruct(y.cpu()))\n",
    "                outputs.append(learn.data.train_ds.y.reconstruct(z.cpu().argmax(1)))\n",
    "    return inputs, targets, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(learn.dl(DatasetType.Valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 8144])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = learn.model.predict(xb[0].cuda()).cpu()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ind = out.argmax(2)\n",
    "out_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9,   1,   1,  ...,   1,   1,   1],\n",
       "        [  9,   1,   1,  ...,   1,   1,   1],\n",
       "        [ 14,  11,  44,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [ 13, 307,   9,  ...,   1,   1,   1],\n",
       "        [  9,   1,   1,  ...,   1,   1,   1],\n",
       "        [ 25, 129,   0,  ...,   1,   1,   1]], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> Note that most of the prediction are made up of just the padding token. Thus the predicitons are quite bad :( </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='151' class='' max='151', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [151/151 01:20<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets, outputs = get_predictions2(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text [9],\n",
       " Text [9],\n",
       " Text [  14   11   44  274   43 2873   82   50 1288   27  704  795   13 4302    9],\n",
       " Text [9],\n",
       " Text [  13  617  131   14   60  131   17   10 1125   14   60  131   17  617  131    9],\n",
       " Text [  14   11   57   37   76  782  782  351 1206   13   10  918   14   10  918   12   76  782    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [103   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24  90  55  10 264  17 216  13 142  31  10 134  12 496   0 190   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18 2559   14 2438   18   14 2438   18   14 2438   18   14 2438   18   14 2438   18   14 2438    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  73  36  42  24 430  13 417  20  10   0  12  10 393  14  73  36  42  24   0   9],\n",
       " Text [9],\n",
       " Text [13 46  9],\n",
       " Text [9],\n",
       " Text [177   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 309   25  325  337 4457   13  161   31   10 1883   15    0    9],\n",
       " Text [9],\n",
       " Text [ 13  84  14 181  13  24   0  13 805  84  83  60   9],\n",
       " Text [9],\n",
       " Text [ 13 142  31  83 201  13  83 867 867 867  14   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 106  23  30   0  32  72   0  14  30 201  13  10  72   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  20 2304    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   25   11   36   42  110   13  511   13   10   72   97   17   46   18   17   19  213   13   10 2076    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 307   9],\n",
       " Text [9],\n",
       " Text [  25  129    0   47   18   43  102   18   61  826  288   18   25   10  619   18  377   50   69 1006    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   10  703   14   10   65  192   12   32  192   40   13  431   23   13  431   23   13 1186    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  33   38 1657   13   19  737    9],\n",
       " Text [9],\n",
       " Text [ 20  32 162  25 162   9],\n",
       " Text [9],\n",
       " Text [ 13 482  60  14  22   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   40 1105  364   18  426 1105 1590   25 1105 1590   18 1105  364   18 1105  364 1416    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 142  10  72 715 715 715 715 715 715 391  17  10 144 793   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 770  10 267 294  12 391   9],\n",
       " Text [9],\n",
       " Text [  13  773   80   57   14   83  867   13 1615   17   10  461  220   80  147   80   98  867    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 222   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 307   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14  146  393   44   23  263   51   53 1690   13   66   83   17  235   12  146  393   14  191    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   25   65   18    0   47   12 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142 8142\n",
       "  8142 8142 8142 8142 8142 8142 8142],\n",
       " Text [ 18 471  14 114  14 114 725 187  18  14  14  14 112 112  14  14 112   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 1332   10  198  312   12   10  198  632   68  312    9],\n",
       " Text [  18    0   18    0   18    0   18    0   18    0   18   14   17   10 1125    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [14 17 10  0  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14  100 1088  931    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 866 436  16 117  13  24   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 1472   10   65 1701   26   82   24  593   13 1472   10  994 1543   12   10 2103  128    9],\n",
       " Text [ 13  84  14   0 121   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [25 50  9],\n",
       " Text [  25   65  753   36   24  117   13   10   72  866 1279    9],\n",
       " Text [56 22  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  36 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685 5685\n",
       "  5685 5685 5685 5685 5685 5685 5685],\n",
       " Text [ 14  11  21  62  95  55  10 266  12  10 759  14  39 228  14  39 228  21  62 658  10 270 304   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56 447  56 447  56 447  70 151  40 289  13  10   0 116   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   11   16   49  327   10  145  235   12 1135  235   12   10 4239   18   14   11   16   10   75  234   12   86\n",
       "   807    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 23  9],\n",
       " Text [9],\n",
       " Text [  56   11   15   32  582  329 1067  115   40  289   13   10 1067   14   56   25   10  130   12 3786    9],\n",
       " Text [9],\n",
       " Text [ 14  65 446   9],\n",
       " Text [ 13 307   9],\n",
       " Text [9],\n",
       " Text [  13  142   31   10 2024   68 1330  623   14   10  897  304   12   10 2024   27 5829 5829 5829 5829 5829 5829 5829\n",
       "  5829 5829 5829 5829    9],\n",
       " Text [13 60 46  9],\n",
       " Text [9],\n",
       " Text [47 14 41 21 62 93  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  142   31   78   14  650 1670   40   10  855  957   13 2787  495  957    9],\n",
       " Text [ 13 540   9],\n",
       " Text [  14   11   15  117   17   10   97 3458   12   10  321   20   10  321   14 1840  112    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  139 1530 1310    9],\n",
       " Text [ 13 457  32 148  14 144 144 144 144 221 683  40 289  13 201  13 201  13 201  14   0 103   9],\n",
       " Text [9],\n",
       " Text [ 14  11  15  32 441 290  40 289  13  10 112  27 299 190  14 103   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  14  11 321  15  10 343  13 491   9],\n",
       " Text [20 32 71  9],\n",
       " Text [ 13 570  66 300  14 131  17 135   9],\n",
       " Text [9],\n",
       " Text [  20   10  198 1315   12  214   14 3471   14 3471   83   60   46    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   19 1945  409    9],\n",
       " Text [9],\n",
       " Text [  13   66  441  433   17   10 2028    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  41  18  10 200  14  10 200  18  34  24  10 200  12  96  12  96 688   9],\n",
       " Text [9],\n",
       " Text [  56   11   15   32  582  329 1067  115   40  289   13   10 1067   14   56   25   10  130   12 3786    9],\n",
       " Text [ 13  32 192  41  62  16  50 593  13 748 121  13 440  32 332  17  10 200   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18 518  18  14  11  36  10   0  18   0  18   0  18   0  18  14   0  18  10   0   9],\n",
       " Text [ 14  11  16  10  60 222  12 617 131  14  11  16  10 369  12  59 222  14 122   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   83 2313   14   97   18   35   24   18   14   13   24   10   83 2313   17   10    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  25   19  363   64   69 2446    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 10  0 54  0  9],\n",
       " Text [9],\n",
       " Text [ 14  73  21  49 101  10   0 256  49 488 314   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   24  104   13  853   10  172   12   10  580  280  257   14   13  142 2046 1543    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   65  123  172   13   24  379   13   65 1588 1588 1005  103    9],\n",
       " Text [9],\n",
       " Text [  14 1075  150   16  195    9],\n",
       " Text [9],\n",
       " Text [ 13  66 247 247  14   0 247   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56 4268   35   24  248   13   24  248   17   10 4670 3329   56 2361    9],\n",
       " Text [  14 1017   21   38   77   13  164 2089   40   76 1017   33   38 2640 1483   13   87    9],\n",
       " Text [  14   11 1381  150   44   23  110   13  142   31  157   16 3604   14  157   17   32  210    9],\n",
       " Text [9],\n",
       " Text [ 14  11 172  13 193  14 193 128  17 235  12  10 592 193   9],\n",
       " Text [9],\n",
       " Text [  14   11  137   77   13   24  133   13 3446   19  952  132  132  132   14  958    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 2541   92   54   19  388  324   54   13 2541   14   24   19  388 1667   20 1268  146  920    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 495  57  28  10 112 266 343   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18 108  18  14 108  18  17  10  72 437  18 108  18  14 108  18  72 931   9],\n",
       " Text [9],\n",
       " Text [  25  325    0   47   63   23 2420   17   10  488   27 1257  681   31   23   67    0   13  616   17 1611    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   19  399   27  227  128   18   15   42   31   15  408   17   19    0   12   10 2806    9],\n",
       " Text [  20  662   14 1031    9],\n",
       " Text [9],\n",
       " Text [ 56 160  35  24 133  40 289  13  32 162  58 188 115   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 20 505 969  14 146 969   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  11  15  10   0  12  10  87  18 426  10 115  18  13 985  14 985  10 337 509   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  25   19 1440   27  318  915  456    9],\n",
       " Text [9],\n",
       " Text [  13 2612   66  528   14  528    9],\n",
       " Text [9],\n",
       " Text [ 13 298   9],\n",
       " Text [  13 1657   13   83 1067    9],\n",
       " Text [ 13  10 785  12   0  12   0  12   0  12  10 409   9],\n",
       " Text [9],\n",
       " Text [ 14 108 883  16 453   0   9],\n",
       " Text [9],\n",
       " Text [ 13  10   0  12  10 198 669  27 197   0  47  12  59 332   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  66 144  14 108  14 144 618  13 482  66 108  14 144 618   9],\n",
       " Text [9],\n",
       " Text [  13    0   14   13    0 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882 6882\n",
       "  6882 6882 6882 6882 6882 6882 6882],\n",
       " Text [9],\n",
       " Text [  14   33   36   10    0   24 4273    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56   19 1262 1262 1262    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 567  17  10 976 174   9],\n",
       " Text [ 14 972   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  431   83 1036   54   19  242   13  431   13 1387    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   11   15   10   81   12   10   78   13  142   31   10 2246   12  688   16 3359  816    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 47  25  50 244  57  31  15 124  13 244  57 124  13 244   9],\n",
       " Text [9],\n",
       " Text [  76 1893    9],\n",
       " Text [ 13 144 294   9],\n",
       " Text [9],\n",
       " Text [ 13 246  14  66  60 203   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 60 18 11 37 42 95  9],\n",
       " Text [ 13 298 686  14 480  19 298 686  12  10 115   9],\n",
       " Text [ 13  60 141 469  14  83  60 141 132   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 359  10  83  18  83  14 482  10  83  17  10  83  17  10 143  12  59 143   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   24   12 1412 1426   12  450   17   10 1052 1412  210    9],\n",
       " Text [  25   22  158   50   24   12  818   18   22  158   10   60   24  585   55 3123    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   10  114 1466 1167   12   10    0   12   10  114 1466 1167  187    9],\n",
       " Text [  56 1327   17   10  879    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 404   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   10   83  445   14  243  445  149   12   59 1471   13   24  243    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 131  17  10 993  12  59 137  17  10 993 131  17  10 342   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56   11  130  997   17   32  666   40 2384   25   17   32  666   40   32 1024    9],\n",
       " Text [9],\n",
       " Text [  13   10  176 1600    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 457  66 414  14 752 656   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 1943 1943   12   46   68 1330 1330 1943   31   15 1330   31 1943   15   50 1330    9],\n",
       " Text [9],\n",
       " Text [  18  377   13 1975   18   13   46   18   34   50   18   43   42   15   50   13   24    0   25   13   24    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  25   50  376 1962    9],\n",
       " Text [9],\n",
       " Text [ 25  50  13  10 613  12 242  31  10 613  15  50 327  13  10 613   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56 2443   47   30   32  310 1010   13   30   32 1010   13   30   32 1212   40   32  148    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25  10 915 201 636   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   40 7781    9],\n",
       " Text [9],\n",
       " Text [  18   25   65 1496    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  32 162   9],\n",
       " Text [ 13  24  12  10   0  33  49  16 923  33  13 457 121   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  481   10 1015  105   12  393   12   29  481  481   10 1015  404   12  186  640    9],\n",
       " Text [  13 2833   14 2833  157    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  52   52  386   47   12   10  633  515   31   23   30 1176   40   32  389    9],\n",
       " Text [9],\n",
       " Text [  13  123   32  441  162   25 1167   14 1167   12 2784    9],\n",
       " Text [ 13 221  14 398   9],\n",
       " Text [9],\n",
       " Text [  13  474   25 2609  349    9],\n",
       " Text [9],\n",
       " Text [ 13  60  66 255   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   60   14 1693    9],\n",
       " Text [ 14 422  13 307   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14  387   16   89   17   10   83   97 1225 1225   12   10    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 306  16  99 663   9],\n",
       " Text [9],\n",
       " Text [  13  106   10   83  141   12   46   14  142   31   10 1330 1330 5125    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  17 1555  103    9],\n",
       " Text [9],\n",
       " Text [  13  481   10  596   12   10 1015  105   12 1015  404   20  186  640  404   17   46    9],\n",
       " Text [ 60 141  46   9],\n",
       " Text [  13  164   17   10 2082   45   10 2082   31   64   69  584   13  164   10  477   45   10  477    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24  12  10 429   0  33  49  16 923  33  13 457 121   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56 105  12 497  15 124  18  32 162  18  31  23 244  10 105  12 497 379  55  10 414 727   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 24  0  9],\n",
       " Text [ 20 109 540   9],\n",
       " Text [ 14  11 185  36  10 414 288 511  13  84 288  14  11 185  36  62 576   9],\n",
       " Text [9],\n",
       " Text [  47   20   23  376   51   51   51   51   52 1962   25  376  122   52 1962   47    9],\n",
       " Text [  13   46  281   42   15  248   13   24  248   13   24    0   13   24 1006   13   10  240  240  240    9],\n",
       " Text [9],\n",
       " Text [ 13  24 881   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25 675 180  67 152 118 135   9],\n",
       " Text [9],\n",
       " Text [ 13  60  46  14  83 141 435   9],\n",
       " Text [ 47  14  22  52  22  47 205  32 210 210  15 323 105  12 437  17  32 210   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25 299  13  46  58 103  17 235  12 100 299  13 743   9],\n",
       " Text [ 14 249  27 278 288   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   66 1291   18  462   14 1291   18   14 1291   18   13  338   66  191  609  298  191  609    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   43  102   18   15   13   74   19  378   12   19  378   12   19 1527  552   18  426    0  202    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 1332  134 2544    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [23  9],\n",
       " Text [9],\n",
       " Text [ 56 165  63  23 376  17  32 341   9],\n",
       " Text [  18   43  102   18   12    0   13  481   10  335   12   85   18   43  102   18  468 1122 1122 1122  968    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24  55  10 176 404  17  10 308  13   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25   0  47  82 301  25   0  19  93 115  12  10   0  19   0   9],\n",
       " Text [9],\n",
       " Text [ 18  41  41  41  38 498  17  10   0  18  41  38 127 166  18  41  41  38 498   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 144 780 428  34  24   0  13 251 922  14 922 922  97 143 143   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13 251 409  14 617 131  14 617 131  14 617 131  14 617 131   9],\n",
       " Text [9],\n",
       " Text [  13  376   10  107  107  107 1059  260  281  837  611  141    9],\n",
       " Text [14 11 57 61 50 90  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   10 3832    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [23  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  26  36  24 219   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  10  57 468   0 650 468   0  12  10 173  57 468 266  12 495   9],\n",
       " Text [  14 1754  103   35   24  287   51   10  505 1446  300    9],\n",
       " Text [ 14 191 596   9],\n",
       " Text [9],\n",
       " Text [ 25 864 445   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   24   79   53  373   79   10 1704   14   13  142   66    0   12  324   14   24 6148    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  359 1693   14   66 1046    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [47  9],\n",
       " Text [9],\n",
       " Text [  13 3471   14 3471   14 3471   46   14 3471   14 3471  596    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56  97 143  35  24 816   9],\n",
       " Text [9],\n",
       " Text [ 13  32 727   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11 155  16  89  31  10 136 155 418  40  10  71  14  11  16  10 136 155 819  29 254   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24 498  19   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  106  177   80   80   80   80   80   80  455  455  455  455  455 1332   86  186    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25  50  13 298   9],\n",
       " Text [9],\n",
       " Text [  13   24   19 2038    0   33  301    0   19    0    0    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   11  434  917  917   16   75  403   13 1181   13   24  346   13   10  583  107  194   29  107    9],\n",
       " Text [ 13 480 146  14  83 214  14  83 214   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   41   21   23 1802    9],\n",
       " Text [9],\n",
       " Text [ 20 733  14 733 543   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13 3471   14    0  393    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11  15  42 491   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 76 148   9],\n",
       " Text [ 20 278 310   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 10  0  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  93  14  11  44  38  21  13 106  42  13 245  13  13 125  19   0 227   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11 150  16  89   9],\n",
       " Text [9],\n",
       " Text [  14   13   11  150   16   17   10  489   13   24  133   13   10  182   14   13  474   10  320   12 1075   14 1075\n",
       "     9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11  15  32 162  58 322  13  10  93  14  11  16 234 547  20  10 755 273   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56   25  252   27  197  332   21   23  534  139   23  338   32 1588   56  395    9],\n",
       " Text [9],\n",
       " Text [  13 1604   10  391   12    0  428   14    0   12   84   78  428    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11 153  12 637  16 117  13 713  11 153  12 637  16   0   9],\n",
       " Text [ 25 422  64  69 133  13 307   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 25 447  44  24 896  13 892 223 193  31  62  30  19 388 193  31  62  30  19 319   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13  0  9],\n",
       " Text [ 14  11  57  21  38  93  40  11  57  14  73  44  38 125  42   9],\n",
       " Text [47  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 146 108   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  56 1327   30   50   69    0    9],\n",
       " Text [ 16 117  13  60 983  14  10 207  31  60 207  16 117  17 381 196   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  14  11 149  21  62  30  18  14  73   9],\n",
       " Text [  14   26   64   13 1499   10  865   31    0   10 2349   30   69    0    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 33  23  30  80  57  28  10 202 847  23  30 735  10  75 922 437   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 87  17 156 228   9],\n",
       " Text [9],\n",
       " Text [ 13 110  17 168 277  14 168 242 327 560 118  66 242 327   9],\n",
       " Text [9],\n",
       " Text [  13 6350   66 1290 1040   14   13 6350   66   66 1290   14  892 1543    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56 666  17  59 130   9],\n",
       " Text [25 50  9],\n",
       " Text [9],\n",
       " Text [14 97  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56 364 516  47  16  17 135  20  10   0  12 543  40   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18   17   32  112 6347   47   21   23  534   16  284   25   77   13   24   19  284  284  911  911  911    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [47  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   24 1124   55   10  112   27   13   27   13   27  318  392    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   26   64   13 1499   10  865   31    0   10 2349   30   69    0    9],\n",
       " Text [ 13  78  17  29 586   9],\n",
       " Text [9],\n",
       " Text [ 25 731 105   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   24   90   55   10   83  111   12   10   72  555   13  892   10 1330   14 1330    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   55   10 1690  237  163   13  930   10 1690   12   10    0  780   17   10  601  250    9],\n",
       " Text [9],\n",
       " Text [  13 2769   10  507  202   31   36  142   31   36  142   31   15  507   13   10  391   12   10   71    9],\n",
       " Text [9],\n",
       " Text [ 56  50 505 171   9],\n",
       " Text [  14   13 1203 1068 2098    9],\n",
       " Text [  18 1462    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 299 150  82  24 152  13 985  10 636 299  13 985  14 299  13  10  97   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  32 103 103 103 103   9],\n",
       " Text [352   9],\n",
       " Text [  14   17   10  144  148  163   13   24 1286   13   10  221   14  148 1664    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  18 1681    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [13 46  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56  65 335   9],\n",
       " Text [9],\n",
       " Text [ 14  11  16  10 232 185  12  10 134  14  11  16  10 232 232 185  20   0   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  11 222  16 195   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 32 148  25  71   9],\n",
       " Text [  56  772   44   24   10   88   12   84  594   43  160   67  297  848  117   51 1347   71    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14  56  25  50 505 435   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  25  32 162   9],\n",
       " Text [9],\n",
       " Text [  14   73   36   62   24  602   13  770   10 1527  393   14 7360    9],\n",
       " Text [9],\n",
       " Text [18 25 50 18  0  9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 56  25  11 560  30  69 152  17  10 162  13 131  10 963 132 115   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 14 114 191  97 185  21  49  77   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24 104   9],\n",
       " Text [  13  616   19  375    0   54   13  616  259   54   19  388 2497   14   13  259    9],\n",
       " Text [9],\n",
       " Text [13 93  9],\n",
       " Text [9],\n",
       " Text [  17 1180   56 1180    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 20  10 533 520  12 157  17 235  12 532  12 141   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14   41   37   10 3460 1057  884    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  24 248  20 102 528 329 232 168 528   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13   53 1210 3536    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 47  13  32 162  25 162   9],\n",
       " Text [ 13 457 718   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 13  32 162   9],\n",
       " Text [9],\n",
       " Text [  25   65  623   44   24  124   13 3301   10  657   12   78   25    0   25   78    9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  14    0   57   34   38  127   57   45    0 2647   12    0    9],\n",
       " Text [  14   26 1370  808    9],\n",
       " Text [ 13 440 414  93   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [  13  139  121 2570   17  486    9],\n",
       " Text [  13  474   32 1624    9],\n",
       " Text [9],\n",
       " Text [ 56 738   9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [9],\n",
       " Text [ 18  14  16  50 117   9],\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> The get_predictions2 function creates the outputs by stripping away the padding token at the end of the predictions. We thus see that most of the sequences consists of just the token with index 9. This is the index for the question-mark. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.vocab.itos[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos que se passerait - il … … si les voyageurs pouvaient renouveler leur passeport en direct et si les entreprises pouvaient remplir les formalités xxunk à partir d’internet ?',\n",
       " 'xxbos what if … … travellers could renew their passports on - line , and businesses could use internet - based customs clearance ?',\n",
       " 'and what would happen if travellers could not re - free travel to innovision ?')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2].text,targets[2].text,outputs[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text [9]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos quelles méthodes a - t - on trouvées particulièrement efficaces pour consulter le public et les parties intéressées sur la protection des renseignements personnels reliés à la santé ?',\n",
       " 'xxbos what approaches have been found particularly effective in consulting with the public and stakeholders on the protection of personal health information ?',\n",
       " '?')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[10].text,targets[10].text,outputs[10].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"xxbos qui a le pouvoir de modifier le règlement sur les poids et mesures et le règlement sur l'inspection de l'électricité et du gaz ?\",\n",
       " 'xxbos who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations ?',\n",
       " 'and what measures are there ?')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[700].text,targets[700].text,outputs[700].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos ´ ` ou sont xxunk leurs grandes convictions en ce qui a trait a la ` ` ´ transparence et a la responsabilite ?',\n",
       " 'xxbos what happened to their great xxunk about transparency and accountability ?',\n",
       " '?')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[701].text,targets[701].text,outputs[701].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos de quoi l’afrique a - t - elle vraiment besoin pour se sortir de la pauvreté ?',\n",
       " 'xxbos what does africa really need to pull itself out of poverty ?',\n",
       " '?')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2500].text,targets[2500].text,outputs[2500].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos quelles ressources votre communauté possède - t - elle qui favoriseraient la guérison ?',\n",
       " 'xxbos what resources exist in your community that would promote recovery ?',\n",
       " '?')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[4002].text,targets[4002].text,outputs[4002].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> Without inputing the true targets for decoder's input, this does not seems to perform very well on translation. Then how come it is considered so good in general?? </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
